{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 单卡 GPU 进行 ChatGLM3-6B 模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora 微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854b989-28a3-48e7-b494-c20c68896bf7",
   "metadata": {},
   "source": [
    "## 0. 安装缺失依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b6fdc7-174f-4053-afe4-df8d60061b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: peft==0.7.1 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (0.7.1)\n",
      "Requirement already satisfied: ruamel_yaml in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (0.18.6)\n",
      "Requirement already satisfied: typer in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (0.12.0)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: rouge_chinese in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (1.0.3)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from peft==0.7.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from peft==0.7.1) (24.0)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from peft==0.7.1) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from peft==0.7.1) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from peft==0.7.1) (2.2.2)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from peft==0.7.1) (4.39.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from peft==0.7.1) (4.66.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from peft==0.7.1) (0.28.0)\n",
      "Requirement already satisfied: safetensors in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from peft==0.7.1) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from peft==0.7.1) (0.22.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from ruamel_yaml) (0.2.8)\n",
      "Requirement already satisfied: typer-slim==0.12.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from typer-slim[standard]==0.12.0->typer) (0.12.0)\n",
      "Requirement already satisfied: typer-cli==0.12.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from typer) (0.12.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from typer-slim==0.12.0->typer-slim[standard]==0.12.0->typer) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from typer-slim==0.12.0->typer-slim[standard]==0.12.0->typer) (4.10.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from typer-slim[standard]==0.12.0->typer) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from typer-slim[standard]==0.12.0->typer) (13.7.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: six in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from rouge_chinese) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.7.1) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.7.1) (12.4.99)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from transformers->peft==0.7.1) (0.15.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from rich>=10.11.0->typer-slim[standard]==0.12.0->typer) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from rich>=10.11.0->typer-slim[standard]==0.12.0->typer) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.7.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft==0.7.1) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer-slim[standard]==0.12.0->typer) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft==0.7.1 ruamel_yaml typer datasets nltk rouge_chinese sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:03<00:00,  2.21it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 114599 examples [00:00, 564749.77 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 1070 examples [00:00, 301262.35 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 1070 examples [00:00, 330479.03 examples/s]\n",
      "Map (num_proc=16): 100%|██████| 114599/114599 [00:05<00:00, 20024.31 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1349.93 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1317.72 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/home/ubuntu/miniconda3/envs/chatglm_finetuning/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.6061, 'grad_norm': 2.680243492126465, 'learning_rate': 4.995e-05, 'epoch': 0.0}\n",
      "{'loss': 3.9824, 'grad_norm': 2.4293534755706787, 'learning_rate': 4.99e-05, 'epoch': 0.0}\n",
      "{'loss': 3.771, 'grad_norm': 2.52138352394104, 'learning_rate': 4.9850000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7063, 'grad_norm': 2.7823281288146973, 'learning_rate': 4.9800000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6939, 'grad_norm': 3.1695048809051514, 'learning_rate': 4.975e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6416, 'grad_norm': 3.532308340072632, 'learning_rate': 4.97e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6407, 'grad_norm': 3.8582763671875, 'learning_rate': 4.965e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6067, 'grad_norm': 4.378069877624512, 'learning_rate': 4.96e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6167, 'grad_norm': 4.476083755493164, 'learning_rate': 4.9550000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6175, 'grad_norm': 4.786456108093262, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5977, 'grad_norm': 4.0952253341674805, 'learning_rate': 4.945e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5588, 'grad_norm': 4.855364799499512, 'learning_rate': 4.94e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5913, 'grad_norm': 4.862276554107666, 'learning_rate': 4.935e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5167, 'grad_norm': 5.120365619659424, 'learning_rate': 4.93e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5387, 'grad_norm': 5.158411026000977, 'learning_rate': 4.9250000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5751, 'grad_norm': 5.2268805503845215, 'learning_rate': 4.92e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5575, 'grad_norm': 6.433966636657715, 'learning_rate': 4.915e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5601, 'grad_norm': 5.265188694000244, 'learning_rate': 4.91e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5656, 'grad_norm': 5.650305271148682, 'learning_rate': 4.905e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5099, 'grad_norm': 5.013583660125732, 'learning_rate': 4.9e-05, 'epoch': 0.04}\n",
      "  2%|▋                                    | 600/30000 [08:07<7:02:21,  1.16it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:20<00:20, 10.28s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:40<00:14, 14.41s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:44<00:00, 10.27s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.650 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 30.144440000000003, 'eval_rouge-2': 6.899839999999999, 'eval_rouge-l': 22.514042, 'eval_bleu-4': 0.030622459461528417, 'eval_runtime': 65.7522, 'eval_samples_per_second': 0.76, 'eval_steps_per_second': 0.061, 'epoch': 0.04}\n",
      "  2%|▋                                    | 600/30000 [09:13<7:02:21,  1.16it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:45<00:00, 10.27s/it]\u001b[A\n",
      "{'loss': 3.4602, 'grad_norm': 5.798738956451416, 'learning_rate': 4.8950000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5535, 'grad_norm': 5.491483211517334, 'learning_rate': 4.89e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4971, 'grad_norm': 6.14288854598999, 'learning_rate': 4.885e-05, 'epoch': 0.05}\n",
      "{'loss': 3.471, 'grad_norm': 5.847026824951172, 'learning_rate': 4.88e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4952, 'grad_norm': 5.45333194732666, 'learning_rate': 4.875e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5257, 'grad_norm': 6.390346050262451, 'learning_rate': 4.87e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5439, 'grad_norm': 5.834984302520752, 'learning_rate': 4.8650000000000003e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5029, 'grad_norm': 5.743723392486572, 'learning_rate': 4.86e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5647, 'grad_norm': 5.7885332107543945, 'learning_rate': 4.855e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5144, 'grad_norm': 6.481152057647705, 'learning_rate': 4.85e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5375, 'grad_norm': 5.686031818389893, 'learning_rate': 4.845e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4945, 'grad_norm': 5.7459917068481445, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4579, 'grad_norm': 6.090282917022705, 'learning_rate': 4.835e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5885, 'grad_norm': 6.263362407684326, 'learning_rate': 4.83e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5059, 'grad_norm': 6.035191535949707, 'learning_rate': 4.825e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5195, 'grad_norm': 7.083225727081299, 'learning_rate': 4.82e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5284, 'grad_norm': 6.058206558227539, 'learning_rate': 4.815e-05, 'epoch': 0.08}\n",
      "{'loss': 3.502, 'grad_norm': 7.348727226257324, 'learning_rate': 4.8100000000000004e-05, 'epoch': 0.08}\n",
      "{'loss': 3.452, 'grad_norm': 6.0832600593566895, 'learning_rate': 4.805e-05, 'epoch': 0.08}\n",
      "{'loss': 3.482, 'grad_norm': 5.870206356048584, 'learning_rate': 4.8e-05, 'epoch': 0.08}\n",
      "  4%|█▍                                  | 1200/30000 [17:15<6:44:53,  1.19it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:10<00:10,  5.05s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:11, 11.39s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.839836000000002, 'eval_rouge-2': 7.163329999999998, 'eval_rouge-l': 24.380212, 'eval_bleu-4': 0.03254240394593804, 'eval_runtime': 54.4162, 'eval_samples_per_second': 0.919, 'eval_steps_per_second': 0.074, 'epoch': 0.08}\n",
      "  4%|█▍                                  | 1200/30000 [18:09<6:44:53,  1.19it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00,  8.28s/it]\u001b[A\n",
      "{'loss': 3.4575, 'grad_norm': 6.026581287384033, 'learning_rate': 4.795e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4866, 'grad_norm': 6.682992935180664, 'learning_rate': 4.79e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5033, 'grad_norm': 5.962779521942139, 'learning_rate': 4.785e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5411, 'grad_norm': 6.450736999511719, 'learning_rate': 4.78e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5054, 'grad_norm': 6.41154670715332, 'learning_rate': 4.775e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5035, 'grad_norm': 6.372785568237305, 'learning_rate': 4.77e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4356, 'grad_norm': 6.451347827911377, 'learning_rate': 4.765e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5224, 'grad_norm': 7.3473358154296875, 'learning_rate': 4.76e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4139, 'grad_norm': 6.992962837219238, 'learning_rate': 4.755e-05, 'epoch': 0.1}\n",
      "{'loss': 3.493, 'grad_norm': 5.9176177978515625, 'learning_rate': 4.75e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5136, 'grad_norm': 6.446647644042969, 'learning_rate': 4.745e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4309, 'grad_norm': 7.423172473907471, 'learning_rate': 4.74e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4985, 'grad_norm': 6.786222457885742, 'learning_rate': 4.735e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4924, 'grad_norm': 6.489490509033203, 'learning_rate': 4.73e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4854, 'grad_norm': 6.231433391571045, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4633, 'grad_norm': 6.44381856918335, 'learning_rate': 4.72e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4273, 'grad_norm': 7.390644073486328, 'learning_rate': 4.715e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4262, 'grad_norm': 7.75681209564209, 'learning_rate': 4.71e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4784, 'grad_norm': 7.408178329467773, 'learning_rate': 4.705e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4146, 'grad_norm': 6.721493244171143, 'learning_rate': 4.7e-05, 'epoch': 0.13}\n",
      "  6%|██▏                                 | 1800/30000 [26:11<6:11:00,  1.27it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:20<00:20, 10.32s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:41<00:14, 14.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.297416000000002, 'eval_rouge-2': 6.712262000000001, 'eval_rouge-l': 23.631146000000005, 'eval_bleu-4': 0.03304614726751203, 'eval_runtime': 80.9902, 'eval_samples_per_second': 0.617, 'eval_steps_per_second': 0.049, 'epoch': 0.13}\n",
      "  6%|██▏                                 | 1800/30000 [27:32<6:11:00,  1.27it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:00<00:00, 16.16s/it]\u001b[A\n",
      "{'loss': 3.4412, 'grad_norm': 7.166315078735352, 'learning_rate': 4.695e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4858, 'grad_norm': 7.2672505378723145, 'learning_rate': 4.69e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3732, 'grad_norm': 6.479464530944824, 'learning_rate': 4.685000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4555, 'grad_norm': 6.7547383308410645, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.13}\n",
      "{'loss': 3.5223, 'grad_norm': 6.378637790679932, 'learning_rate': 4.6750000000000005e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4771, 'grad_norm': 6.896148681640625, 'learning_rate': 4.6700000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4967, 'grad_norm': 6.919339179992676, 'learning_rate': 4.665e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4509, 'grad_norm': 7.4681549072265625, 'learning_rate': 4.660000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4795, 'grad_norm': 6.208704471588135, 'learning_rate': 4.655000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4719, 'grad_norm': 6.685189723968506, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4838, 'grad_norm': 6.6244940757751465, 'learning_rate': 4.6450000000000004e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3934, 'grad_norm': 7.545914649963379, 'learning_rate': 4.64e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4378, 'grad_norm': 7.035646915435791, 'learning_rate': 4.635e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3996, 'grad_norm': 6.781537055969238, 'learning_rate': 4.630000000000001e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4721, 'grad_norm': 7.132479667663574, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.16}\n",
      "{'loss': 3.433, 'grad_norm': 7.959497451782227, 'learning_rate': 4.6200000000000005e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4441, 'grad_norm': 7.390660762786865, 'learning_rate': 4.6150000000000004e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5141, 'grad_norm': 6.778231143951416, 'learning_rate': 4.61e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5031, 'grad_norm': 7.052666664123535, 'learning_rate': 4.605e-05, 'epoch': 0.17}\n",
      "{'loss': 3.49, 'grad_norm': 7.715889930725098, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.17}\n",
      "  8%|██▉                                 | 2400/30000 [35:36<6:02:06,  1.27it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:21<00:21, 10.71s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:42<00:15, 15.07s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.218206, 'eval_rouge-2': 6.5549, 'eval_rouge-l': 24.326612, 'eval_bleu-4': 0.03140645751907633, 'eval_runtime': 49.0228, 'eval_samples_per_second': 1.02, 'eval_steps_per_second': 0.082, 'epoch': 0.17}\n",
      "  8%|██▉                                 | 2400/30000 [36:25<6:02:06,  1.27it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:45<00:00, 10.42s/it]\u001b[A\n",
      "{'loss': 3.4138, 'grad_norm': 7.193683624267578, 'learning_rate': 4.5950000000000006e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4559, 'grad_norm': 7.604886531829834, 'learning_rate': 4.5900000000000004e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4278, 'grad_norm': 7.053867340087891, 'learning_rate': 4.585e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4001, 'grad_norm': 7.924333572387695, 'learning_rate': 4.58e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4863, 'grad_norm': 6.800868988037109, 'learning_rate': 4.575e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4104, 'grad_norm': 7.445026874542236, 'learning_rate': 4.5700000000000006e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4688, 'grad_norm': 6.64879846572876, 'learning_rate': 4.5650000000000005e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3728, 'grad_norm': 6.795741558074951, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.18}\n",
      "{'loss': 3.383, 'grad_norm': 6.740181922912598, 'learning_rate': 4.555e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3936, 'grad_norm': 7.368183612823486, 'learning_rate': 4.55e-05, 'epoch': 0.19}\n",
      "{'loss': 3.462, 'grad_norm': 8.239100456237793, 'learning_rate': 4.545000000000001e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4169, 'grad_norm': 7.03926944732666, 'learning_rate': 4.5400000000000006e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4391, 'grad_norm': 7.34965705871582, 'learning_rate': 4.5350000000000005e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4313, 'grad_norm': 7.406957149505615, 'learning_rate': 4.53e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4695, 'grad_norm': 8.030553817749023, 'learning_rate': 4.525e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4919, 'grad_norm': 7.15851354598999, 'learning_rate': 4.52e-05, 'epoch': 0.2}\n",
      "{'loss': 3.426, 'grad_norm': 7.6003265380859375, 'learning_rate': 4.5150000000000006e-05, 'epoch': 0.2}\n",
      "{'loss': 3.363, 'grad_norm': 6.611905574798584, 'learning_rate': 4.5100000000000005e-05, 'epoch': 0.21}\n",
      "{'loss': 3.465, 'grad_norm': 6.5580573081970215, 'learning_rate': 4.5050000000000004e-05, 'epoch': 0.21}\n",
      "{'loss': 3.5019, 'grad_norm': 7.163265228271484, 'learning_rate': 4.5e-05, 'epoch': 0.21}\n",
      " 10%|███▌                                | 3000/30000 [44:32<6:26:27,  1.16it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:20<00:20, 10.33s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:40<00:14, 14.48s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.458568, 'eval_rouge-2': 7.533141999999999, 'eval_rouge-l': 23.695628000000006, 'eval_bleu-4': 0.031209905274506927, 'eval_runtime': 81.033, 'eval_samples_per_second': 0.617, 'eval_steps_per_second': 0.049, 'epoch': 0.21}\n",
      " 10%|███▌                                | 3000/30000 [45:53<6:26:27,  1.16it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:00<00:00, 16.18s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-3000\n",
      "tokenizer config file saved in ./output/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 3.3783, 'grad_norm': 8.126749038696289, 'learning_rate': 4.495e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4402, 'grad_norm': 8.64502239227295, 'learning_rate': 4.49e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4522, 'grad_norm': 7.879504680633545, 'learning_rate': 4.4850000000000006e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4167, 'grad_norm': 6.660201072692871, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4169, 'grad_norm': 7.929840087890625, 'learning_rate': 4.4750000000000004e-05, 'epoch': 0.22}\n",
      "{'loss': 3.499, 'grad_norm': 8.275854110717773, 'learning_rate': 4.47e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4753, 'grad_norm': 7.700952053070068, 'learning_rate': 4.465e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4802, 'grad_norm': 7.001598834991455, 'learning_rate': 4.46e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4131, 'grad_norm': 6.731331825256348, 'learning_rate': 4.4550000000000005e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4666, 'grad_norm': 7.21730899810791, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3756, 'grad_norm': 7.665393352508545, 'learning_rate': 4.445e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4408, 'grad_norm': 7.865622520446777, 'learning_rate': 4.44e-05, 'epoch': 0.23}\n",
      "{'loss': 3.429, 'grad_norm': 7.265857696533203, 'learning_rate': 4.435e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4282, 'grad_norm': 7.949788570404053, 'learning_rate': 4.43e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4207, 'grad_norm': 7.297287464141846, 'learning_rate': 4.4250000000000005e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4128, 'grad_norm': 8.154088973999023, 'learning_rate': 4.4200000000000004e-05, 'epoch': 0.24}\n",
      "{'loss': 3.5067, 'grad_norm': 7.509565830230713, 'learning_rate': 4.415e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3677, 'grad_norm': 8.435234069824219, 'learning_rate': 4.41e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4471, 'grad_norm': 8.045578002929688, 'learning_rate': 4.405e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4365, 'grad_norm': 8.740306854248047, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.25}\n",
      " 12%|████▎                               | 3600/30000 [53:57<5:56:50,  1.23it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:20<00:20, 10.30s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:07,  7.60s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.02546, 'eval_rouge-2': 7.0349639999999996, 'eval_rouge-l': 25.128235999999998, 'eval_bleu-4': 0.035064354152382536, 'eval_runtime': 32.0679, 'eval_samples_per_second': 1.559, 'eval_steps_per_second': 0.125, 'epoch': 0.25}\n",
      " 12%|████▎                               | 3600/30000 [54:29<5:56:50,  1.23it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:27<00:00,  5.66s/it]\u001b[A\n",
      "{'loss': 3.3695, 'grad_norm': 8.176431655883789, 'learning_rate': 4.3950000000000004e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4371, 'grad_norm': 6.9575419425964355, 'learning_rate': 4.39e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3919, 'grad_norm': 7.830154895782471, 'learning_rate': 4.385e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4406, 'grad_norm': 7.872459888458252, 'learning_rate': 4.38e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3248, 'grad_norm': 7.698606014251709, 'learning_rate': 4.375e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4594, 'grad_norm': 7.585097789764404, 'learning_rate': 4.3700000000000005e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4441, 'grad_norm': 7.872668743133545, 'learning_rate': 4.3650000000000004e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4159, 'grad_norm': 7.0704827308654785, 'learning_rate': 4.36e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4332, 'grad_norm': 7.67720890045166, 'learning_rate': 4.355e-05, 'epoch': 0.27}\n",
      "{'loss': 3.409, 'grad_norm': 6.85394287109375, 'learning_rate': 4.35e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4646, 'grad_norm': 7.1889848709106445, 'learning_rate': 4.345e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3995, 'grad_norm': 7.34989595413208, 'learning_rate': 4.3400000000000005e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4014, 'grad_norm': 7.752035617828369, 'learning_rate': 4.335e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3597, 'grad_norm': 8.366921424865723, 'learning_rate': 4.33e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3906, 'grad_norm': 7.536273002624512, 'learning_rate': 4.325e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4301, 'grad_norm': 7.759682655334473, 'learning_rate': 4.32e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4417, 'grad_norm': 8.560110092163086, 'learning_rate': 4.315e-05, 'epoch': 0.29}\n",
      "{'loss': 3.4114, 'grad_norm': 6.9182305335998535, 'learning_rate': 4.3100000000000004e-05, 'epoch': 0.29}\n",
      "{'loss': 3.4362, 'grad_norm': 8.278127670288086, 'learning_rate': 4.305e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3711, 'grad_norm': 8.324469566345215, 'learning_rate': 4.3e-05, 'epoch': 0.29}\n",
      " 14%|████▊                             | 4200/30000 [1:02:36<5:42:53,  1.25it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.13s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.62s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.969954, 'eval_rouge-2': 6.941808, 'eval_rouge-l': 25.437783999999997, 'eval_bleu-4': 0.0329948611874987, 'eval_runtime': 12.8204, 'eval_samples_per_second': 3.9, 'eval_steps_per_second': 0.312, 'epoch': 0.29}\n",
      " 14%|████▊                             | 4200/30000 [1:02:49<5:42:53,  1.25it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.44s/it]\u001b[A\n",
      "{'loss': 3.3767, 'grad_norm': 7.949723720550537, 'learning_rate': 4.295e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3937, 'grad_norm': 7.316962242126465, 'learning_rate': 4.29e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4327, 'grad_norm': 7.846226692199707, 'learning_rate': 4.285e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4319, 'grad_norm': 7.501446723937988, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4247, 'grad_norm': 7.6695146560668945, 'learning_rate': 4.275e-05, 'epoch': 0.3}\n",
      "{'loss': 3.4007, 'grad_norm': 7.298162460327148, 'learning_rate': 4.27e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4106, 'grad_norm': 7.15922212600708, 'learning_rate': 4.265e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4188, 'grad_norm': 7.291126728057861, 'learning_rate': 4.26e-05, 'epoch': 0.31}\n",
      "{'loss': 3.402, 'grad_norm': 7.648678302764893, 'learning_rate': 4.2550000000000004e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4512, 'grad_norm': 8.47422981262207, 'learning_rate': 4.25e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3749, 'grad_norm': 6.913898944854736, 'learning_rate': 4.245e-05, 'epoch': 0.32}\n",
      "{'loss': 3.4048, 'grad_norm': 6.856907367706299, 'learning_rate': 4.24e-05, 'epoch': 0.32}\n",
      "{'loss': 3.358, 'grad_norm': 7.6920366287231445, 'learning_rate': 4.235e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3943, 'grad_norm': 7.377283573150635, 'learning_rate': 4.23e-05, 'epoch': 0.32}\n",
      "{'loss': 3.4049, 'grad_norm': 7.459948539733887, 'learning_rate': 4.2250000000000004e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3984, 'grad_norm': 8.370185852050781, 'learning_rate': 4.22e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3636, 'grad_norm': 7.63299560546875, 'learning_rate': 4.215e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3695, 'grad_norm': 7.068983554840088, 'learning_rate': 4.21e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3723, 'grad_norm': 8.226805686950684, 'learning_rate': 4.205e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4111, 'grad_norm': 7.761397838592529, 'learning_rate': 4.2e-05, 'epoch': 0.34}\n",
      " 16%|█████▍                            | 4800/30000 [1:10:51<5:34:06,  1.26it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:20<00:20, 10.23s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:40<00:14, 14.37s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.910684, 'eval_rouge-2': 7.116331999999999, 'eval_rouge-l': 22.210048, 'eval_bleu-4': 0.03093009989550983, 'eval_runtime': 80.2046, 'eval_samples_per_second': 0.623, 'eval_steps_per_second': 0.05, 'epoch': 0.34}\n",
      " 16%|█████▍                            | 4800/30000 [1:12:11<5:34:06,  1.26it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:59<00:00, 15.97s/it]\u001b[A\n",
      "{'loss': 3.4283, 'grad_norm': 7.888555526733398, 'learning_rate': 4.195e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4398, 'grad_norm': 8.193319320678711, 'learning_rate': 4.19e-05, 'epoch': 0.34}\n",
      "{'loss': 3.46, 'grad_norm': 7.974674701690674, 'learning_rate': 4.185e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3391, 'grad_norm': 7.706533432006836, 'learning_rate': 4.18e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3392, 'grad_norm': 7.973882675170898, 'learning_rate': 4.175e-05, 'epoch': 0.35}\n",
      "{'loss': 3.4159, 'grad_norm': 8.41360855102539, 'learning_rate': 4.17e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3835, 'grad_norm': 8.21941089630127, 'learning_rate': 4.165e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3796, 'grad_norm': 8.302017211914062, 'learning_rate': 4.16e-05, 'epoch': 0.35}\n",
      "{'loss': 3.4019, 'grad_norm': 7.312986850738525, 'learning_rate': 4.155e-05, 'epoch': 0.35}\n",
      "{'loss': 3.392, 'grad_norm': 7.387391090393066, 'learning_rate': 4.15e-05, 'epoch': 0.36}\n",
      "{'loss': 3.389, 'grad_norm': 6.9115519523620605, 'learning_rate': 4.145e-05, 'epoch': 0.36}\n",
      "{'loss': 3.4243, 'grad_norm': 7.469374179840088, 'learning_rate': 4.14e-05, 'epoch': 0.36}\n",
      "{'loss': 3.4383, 'grad_norm': 7.502997875213623, 'learning_rate': 4.135e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3406, 'grad_norm': 7.508111476898193, 'learning_rate': 4.13e-05, 'epoch': 0.36}\n",
      "{'loss': 3.4106, 'grad_norm': 7.805782794952393, 'learning_rate': 4.125e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3706, 'grad_norm': 7.721933364868164, 'learning_rate': 4.12e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3758, 'grad_norm': 7.254140853881836, 'learning_rate': 4.115e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3557, 'grad_norm': 7.454056262969971, 'learning_rate': 4.11e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3455, 'grad_norm': 7.511575222015381, 'learning_rate': 4.105e-05, 'epoch': 0.37}\n",
      "{'loss': 3.4318, 'grad_norm': 8.052111625671387, 'learning_rate': 4.1e-05, 'epoch': 0.38}\n",
      " 18%|██████                            | 5400/30000 [1:20:15<5:18:53,  1.29it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:20<00:20, 10.33s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:40<00:14, 14.47s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.624754, 'eval_rouge-2': 7.873668, 'eval_rouge-l': 23.839807999999998, 'eval_bleu-4': 0.03717590950403732, 'eval_runtime': 64.5226, 'eval_samples_per_second': 0.775, 'eval_steps_per_second': 0.062, 'epoch': 0.38}\n",
      " 18%|██████                            | 5400/30000 [1:21:20<5:18:53,  1.29it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:43<00:00, 10.03s/it]\u001b[A\n",
      "{'loss': 3.4015, 'grad_norm': 8.591469764709473, 'learning_rate': 4.095e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3609, 'grad_norm': 8.151880264282227, 'learning_rate': 4.09e-05, 'epoch': 0.38}\n",
      "{'loss': 3.384, 'grad_norm': 8.392688751220703, 'learning_rate': 4.085e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3257, 'grad_norm': 7.965723991394043, 'learning_rate': 4.08e-05, 'epoch': 0.39}\n",
      "{'loss': 3.4145, 'grad_norm': 8.217924118041992, 'learning_rate': 4.075e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3652, 'grad_norm': 9.033284187316895, 'learning_rate': 4.07e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3721, 'grad_norm': 7.414141654968262, 'learning_rate': 4.065e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3838, 'grad_norm': 8.656784057617188, 'learning_rate': 4.0600000000000004e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3757, 'grad_norm': 8.300816535949707, 'learning_rate': 4.055e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3885, 'grad_norm': 7.967799663543701, 'learning_rate': 4.05e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3594, 'grad_norm': 8.057441711425781, 'learning_rate': 4.045000000000001e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3982, 'grad_norm': 8.452089309692383, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.4}\n",
      "{'loss': 3.4311, 'grad_norm': 8.021368980407715, 'learning_rate': 4.0350000000000005e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3801, 'grad_norm': 7.7630228996276855, 'learning_rate': 4.0300000000000004e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3266, 'grad_norm': 7.8525919914245605, 'learning_rate': 4.025e-05, 'epoch': 0.41}\n",
      "{'loss': 3.346, 'grad_norm': 8.301737785339355, 'learning_rate': 4.02e-05, 'epoch': 0.41}\n",
      "{'loss': 3.4029, 'grad_norm': 7.92752742767334, 'learning_rate': 4.015000000000001e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3579, 'grad_norm': 7.5561909675598145, 'learning_rate': 4.0100000000000006e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3259, 'grad_norm': 7.565214157104492, 'learning_rate': 4.0050000000000004e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3736, 'grad_norm': 10.988565444946289, 'learning_rate': 4e-05, 'epoch': 0.42}\n",
      " 20%|██████▊                           | 6000/30000 [1:29:21<5:24:20,  1.23it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:20<00:20, 10.30s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:07,  7.71s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.137662, 'eval_rouge-2': 6.768130000000001, 'eval_rouge-l': 25.079728, 'eval_bleu-4': 0.034959065151222574, 'eval_runtime': 30.1797, 'eval_samples_per_second': 1.657, 'eval_steps_per_second': 0.133, 'epoch': 0.42}\n",
      " 20%|██████▊                           | 6000/30000 [1:29:51<5:24:20,  1.23it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:27<00:00,  5.70s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-6000\n",
      "tokenizer config file saved in ./output/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 3.3539, 'grad_norm': 8.434160232543945, 'learning_rate': 3.995e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3333, 'grad_norm': 8.056111335754395, 'learning_rate': 3.99e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3151, 'grad_norm': 7.211853504180908, 'learning_rate': 3.9850000000000006e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3679, 'grad_norm': 7.6668596267700195, 'learning_rate': 3.9800000000000005e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3719, 'grad_norm': 7.629093647003174, 'learning_rate': 3.9750000000000004e-05, 'epoch': 0.43}\n",
      "{'loss': 3.4062, 'grad_norm': 7.995259761810303, 'learning_rate': 3.97e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3003, 'grad_norm': 7.699855804443359, 'learning_rate': 3.965e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3934, 'grad_norm': 8.751587867736816, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3863, 'grad_norm': 8.565178871154785, 'learning_rate': 3.9550000000000006e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3582, 'grad_norm': 7.744260787963867, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3689, 'grad_norm': 7.649402141571045, 'learning_rate': 3.9450000000000003e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3079, 'grad_norm': 7.579432487487793, 'learning_rate': 3.94e-05, 'epoch': 0.44}\n",
      "{'loss': 3.373, 'grad_norm': 8.635621070861816, 'learning_rate': 3.935e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3508, 'grad_norm': 8.502504348754883, 'learning_rate': 3.9300000000000007e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3333, 'grad_norm': 8.287149429321289, 'learning_rate': 3.9250000000000005e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3217, 'grad_norm': 7.571360111236572, 'learning_rate': 3.9200000000000004e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3639, 'grad_norm': 8.194414138793945, 'learning_rate': 3.915e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3322, 'grad_norm': 7.616128921508789, 'learning_rate': 3.91e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3676, 'grad_norm': 7.605559825897217, 'learning_rate': 3.905e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3727, 'grad_norm': 7.8842692375183105, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.46}\n",
      " 22%|███████▍                          | 6600/30000 [1:37:57<5:28:22,  1.19it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:20<00:20, 10.26s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:40<00:14, 14.38s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.08709400000001, 'eval_rouge-2': 7.645576, 'eval_rouge-l': 24.016776, 'eval_bleu-4': 0.03519258860931096, 'eval_runtime': 64.3639, 'eval_samples_per_second': 0.777, 'eval_steps_per_second': 0.062, 'epoch': 0.46}\n",
      " 22%|███████▍                          | 6600/30000 [1:39:01<5:28:22,  1.19it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:43<00:00, 10.08s/it]\u001b[A\n",
      "{'loss': 3.3496, 'grad_norm': 8.158951759338379, 'learning_rate': 3.8950000000000005e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3335, 'grad_norm': 7.364867210388184, 'learning_rate': 3.8900000000000004e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3746, 'grad_norm': 7.871110916137695, 'learning_rate': 3.885e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3878, 'grad_norm': 7.789318561553955, 'learning_rate': 3.88e-05, 'epoch': 0.47}\n",
      "{'loss': 3.363, 'grad_norm': 9.110013008117676, 'learning_rate': 3.875e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3883, 'grad_norm': 7.632988452911377, 'learning_rate': 3.8700000000000006e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3396, 'grad_norm': 7.79097843170166, 'learning_rate': 3.8650000000000004e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3609, 'grad_norm': 7.388261318206787, 'learning_rate': 3.86e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3566, 'grad_norm': 7.653175354003906, 'learning_rate': 3.855e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3258, 'grad_norm': 8.505894660949707, 'learning_rate': 3.85e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3624, 'grad_norm': 7.770918846130371, 'learning_rate': 3.845e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3608, 'grad_norm': 8.296562194824219, 'learning_rate': 3.8400000000000005e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3818, 'grad_norm': 7.4699482917785645, 'learning_rate': 3.8350000000000004e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3226, 'grad_norm': 8.75764274597168, 'learning_rate': 3.83e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3238, 'grad_norm': 8.434541702270508, 'learning_rate': 3.825e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3577, 'grad_norm': 8.492372512817383, 'learning_rate': 3.82e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3188, 'grad_norm': 7.790838241577148, 'learning_rate': 3.8150000000000006e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3447, 'grad_norm': 7.986880779266357, 'learning_rate': 3.8100000000000005e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3399, 'grad_norm': 7.9221086502075195, 'learning_rate': 3.805e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3274, 'grad_norm': 8.422810554504395, 'learning_rate': 3.8e-05, 'epoch': 0.5}\n",
      " 24%|████████▏                         | 7200/30000 [1:47:05<5:09:08,  1.23it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:09<00:09,  4.67s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.27s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.58819200000001, 'eval_rouge-2': 7.683494, 'eval_rouge-l': 24.815326, 'eval_bleu-4': 0.03840685830048135, 'eval_runtime': 21.131, 'eval_samples_per_second': 2.366, 'eval_steps_per_second': 0.189, 'epoch': 0.5}\n",
      " 24%|████████▏                         | 7200/30000 [1:47:26<5:09:08,  1.23it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:16<00:00,  3.75s/it]\u001b[A\n",
      "{'loss': 3.354, 'grad_norm': 7.75876522064209, 'learning_rate': 3.795e-05, 'epoch': 0.5}\n",
      "{'loss': 3.3604, 'grad_norm': 7.951098918914795, 'learning_rate': 3.79e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3581, 'grad_norm': 7.905972480773926, 'learning_rate': 3.7850000000000005e-05, 'epoch': 0.51}\n",
      "{'loss': 3.276, 'grad_norm': 8.708250999450684, 'learning_rate': 3.7800000000000004e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3262, 'grad_norm': 7.520483016967773, 'learning_rate': 3.775e-05, 'epoch': 0.51}\n",
      "{'loss': 3.4064, 'grad_norm': 8.712449073791504, 'learning_rate': 3.77e-05, 'epoch': 0.52}\n",
      "{'loss': 3.4017, 'grad_norm': 7.620131969451904, 'learning_rate': 3.765e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3667, 'grad_norm': 8.490316390991211, 'learning_rate': 3.76e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3456, 'grad_norm': 8.693753242492676, 'learning_rate': 3.7550000000000005e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3867, 'grad_norm': 8.036916732788086, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3467, 'grad_norm': 8.976107597351074, 'learning_rate': 3.745e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3334, 'grad_norm': 8.129639625549316, 'learning_rate': 3.74e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2867, 'grad_norm': 8.419875144958496, 'learning_rate': 3.735e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3497, 'grad_norm': 8.453290939331055, 'learning_rate': 3.73e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3117, 'grad_norm': 8.827971458435059, 'learning_rate': 3.7250000000000004e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3652, 'grad_norm': 8.094736099243164, 'learning_rate': 3.72e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3642, 'grad_norm': 8.129063606262207, 'learning_rate': 3.715e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3934, 'grad_norm': 7.7195143699646, 'learning_rate': 3.71e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3337, 'grad_norm': 8.292534828186035, 'learning_rate': 3.705e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2952, 'grad_norm': 8.602753639221191, 'learning_rate': 3.7e-05, 'epoch': 0.54}\n",
      " 26%|████████▊                         | 7800/30000 [1:55:27<4:51:11,  1.27it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:08<00:08,  4.39s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.38s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.065984, 'eval_rouge-2': 7.588334000000001, 'eval_rouge-l': 25.544169999999998, 'eval_bleu-4': 0.03516185234664376, 'eval_runtime': 18.6436, 'eval_samples_per_second': 2.682, 'eval_steps_per_second': 0.215, 'epoch': 0.54}\n",
      " 26%|████████▊                         | 7800/30000 [1:55:46<4:51:11,  1.27it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.58s/it]\u001b[A\n",
      "{'loss': 3.4024, 'grad_norm': 8.528470039367676, 'learning_rate': 3.6950000000000004e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3301, 'grad_norm': 8.000372886657715, 'learning_rate': 3.69e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3766, 'grad_norm': 8.386791229248047, 'learning_rate': 3.685e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3041, 'grad_norm': 8.000956535339355, 'learning_rate': 3.68e-05, 'epoch': 0.55}\n",
      "{'loss': 3.4165, 'grad_norm': 9.082761764526367, 'learning_rate': 3.675e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3475, 'grad_norm': 8.963874816894531, 'learning_rate': 3.6700000000000004e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2618, 'grad_norm': 7.960196495056152, 'learning_rate': 3.665e-05, 'epoch': 0.56}\n",
      "{'loss': 3.3405, 'grad_norm': 9.477324485778809, 'learning_rate': 3.66e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2898, 'grad_norm': 8.315939903259277, 'learning_rate': 3.655e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2427, 'grad_norm': 7.946921348571777, 'learning_rate': 3.65e-05, 'epoch': 0.57}\n",
      "{'loss': 3.282, 'grad_norm': 8.552974700927734, 'learning_rate': 3.645e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3434, 'grad_norm': 8.21127986907959, 'learning_rate': 3.6400000000000004e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3367, 'grad_norm': 7.911695957183838, 'learning_rate': 3.635e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3246, 'grad_norm': 8.41420841217041, 'learning_rate': 3.63e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3383, 'grad_norm': 8.435962677001953, 'learning_rate': 3.625e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3107, 'grad_norm': 8.391090393066406, 'learning_rate': 3.62e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3189, 'grad_norm': 7.793842792510986, 'learning_rate': 3.615e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3624, 'grad_norm': 8.401235580444336, 'learning_rate': 3.61e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3786, 'grad_norm': 8.303850173950195, 'learning_rate': 3.605e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3534, 'grad_norm': 9.596890449523926, 'learning_rate': 3.6e-05, 'epoch': 0.59}\n",
      " 28%|█████████▌                        | 8400/30000 [2:03:52<4:32:50,  1.32it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:21<00:21, 10.71s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:42<00:15, 15.08s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.626556, 'eval_rouge-2': 7.147412, 'eval_rouge-l': 22.699522, 'eval_bleu-4': 0.03340869562978007, 'eval_runtime': 67.7946, 'eval_samples_per_second': 0.738, 'eval_steps_per_second': 0.059, 'epoch': 0.59}\n",
      " 28%|█████████▌                        | 8400/30000 [2:05:00<4:32:50,  1.32it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:46<00:00, 10.68s/it]\u001b[A\n",
      "{'loss': 3.3108, 'grad_norm': 8.985148429870605, 'learning_rate': 3.595e-05, 'epoch': 0.59}\n",
      "{'loss': 3.3429, 'grad_norm': 8.029658317565918, 'learning_rate': 3.59e-05, 'epoch': 0.59}\n",
      "{'loss': 3.425, 'grad_norm': 9.650372505187988, 'learning_rate': 3.585e-05, 'epoch': 0.59}\n",
      "{'loss': 3.3096, 'grad_norm': 8.534420013427734, 'learning_rate': 3.58e-05, 'epoch': 0.59}\n",
      "{'loss': 3.3949, 'grad_norm': 8.237565040588379, 'learning_rate': 3.575e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3103, 'grad_norm': 8.726933479309082, 'learning_rate': 3.57e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2669, 'grad_norm': 9.255325317382812, 'learning_rate': 3.565e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3373, 'grad_norm': 9.089813232421875, 'learning_rate': 3.56e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3103, 'grad_norm': 9.002564430236816, 'learning_rate': 3.555e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3715, 'grad_norm': 8.57076644897461, 'learning_rate': 3.55e-05, 'epoch': 0.61}\n",
      "{'loss': 3.333, 'grad_norm': 9.392176628112793, 'learning_rate': 3.545e-05, 'epoch': 0.61}\n",
      "{'loss': 3.4115, 'grad_norm': 9.281681060791016, 'learning_rate': 3.54e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3118, 'grad_norm': 8.316844940185547, 'learning_rate': 3.535e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2885, 'grad_norm': 8.129669189453125, 'learning_rate': 3.53e-05, 'epoch': 0.62}\n",
      "{'loss': 3.31, 'grad_norm': 7.9734206199646, 'learning_rate': 3.525e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3382, 'grad_norm': 7.618133544921875, 'learning_rate': 3.52e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3724, 'grad_norm': 8.229143142700195, 'learning_rate': 3.515e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3872, 'grad_norm': 9.201868057250977, 'learning_rate': 3.51e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3161, 'grad_norm': 8.55544662475586, 'learning_rate': 3.505e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2967, 'grad_norm': 8.698219299316406, 'learning_rate': 3.5e-05, 'epoch': 0.63}\n",
      " 30%|██████████▏                       | 9000/30000 [2:13:02<4:40:08,  1.25it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:20<00:20, 10.25s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:40<00:14, 14.37s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.11085, 'eval_rouge-2': 7.94239, 'eval_rouge-l': 23.971743999999997, 'eval_bleu-4': 0.038001441540434173, 'eval_runtime': 80.2628, 'eval_samples_per_second': 0.623, 'eval_steps_per_second': 0.05, 'epoch': 0.63}\n",
      " 30%|██████████▏                       | 9000/30000 [2:14:22<4:40:08,  1.25it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:59<00:00, 15.97s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-9000\n",
      "tokenizer config file saved in ./output/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 3.4012, 'grad_norm': 8.614980697631836, 'learning_rate': 3.495e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3917, 'grad_norm': 8.565401077270508, 'learning_rate': 3.49e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3782, 'grad_norm': 8.025175094604492, 'learning_rate': 3.485e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3492, 'grad_norm': 8.276778221130371, 'learning_rate': 3.48e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3621, 'grad_norm': 8.456701278686523, 'learning_rate': 3.475e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3185, 'grad_norm': 8.37832260131836, 'learning_rate': 3.4699999999999996e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3359, 'grad_norm': 7.631916046142578, 'learning_rate': 3.465e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3469, 'grad_norm': 9.115440368652344, 'learning_rate': 3.46e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2906, 'grad_norm': 8.313509941101074, 'learning_rate': 3.455e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3587, 'grad_norm': 8.42981243133545, 'learning_rate': 3.45e-05, 'epoch': 0.65}\n",
      "{'loss': 3.299, 'grad_norm': 8.503800392150879, 'learning_rate': 3.445e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3358, 'grad_norm': 8.827361106872559, 'learning_rate': 3.4399999999999996e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3249, 'grad_norm': 7.789554595947266, 'learning_rate': 3.435e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3706, 'grad_norm': 7.798308372497559, 'learning_rate': 3.430000000000001e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3658, 'grad_norm': 8.789915084838867, 'learning_rate': 3.4250000000000006e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2898, 'grad_norm': 9.088077545166016, 'learning_rate': 3.4200000000000005e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2991, 'grad_norm': 8.83453369140625, 'learning_rate': 3.415e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3593, 'grad_norm': 7.557278633117676, 'learning_rate': 3.41e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2772, 'grad_norm': 8.09862232208252, 'learning_rate': 3.405e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3255, 'grad_norm': 9.067309379577637, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.67}\n",
      " 32%|██████████▉                       | 9600/30000 [2:22:25<4:45:58,  1.19it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:20<00:20, 10.29s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:23<00:07,  7.20s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.233496, 'eval_rouge-2': 7.226156, 'eval_rouge-l': 24.740634, 'eval_bleu-4': 0.0370734623716072, 'eval_runtime': 29.9478, 'eval_samples_per_second': 1.67, 'eval_steps_per_second': 0.134, 'epoch': 0.67}\n",
      " 32%|██████████▉                       | 9600/30000 [2:22:55<4:45:58,  1.19it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:26<00:00,  5.55s/it]\u001b[A\n",
      "{'loss': 3.3212, 'grad_norm': 8.635419845581055, 'learning_rate': 3.3950000000000005e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3217, 'grad_norm': 8.711919784545898, 'learning_rate': 3.3900000000000004e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3119, 'grad_norm': 8.407095909118652, 'learning_rate': 3.385e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2888, 'grad_norm': 8.641767501831055, 'learning_rate': 3.38e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2942, 'grad_norm': 8.049359321594238, 'learning_rate': 3.375000000000001e-05, 'epoch': 0.68}\n",
      "{'loss': 3.3049, 'grad_norm': 8.060670852661133, 'learning_rate': 3.3700000000000006e-05, 'epoch': 0.68}\n",
      "{'loss': 3.289, 'grad_norm': 8.617500305175781, 'learning_rate': 3.3650000000000005e-05, 'epoch': 0.68}\n",
      "{'loss': 3.3668, 'grad_norm': 9.513184547424316, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3157, 'grad_norm': 8.202520370483398, 'learning_rate': 3.355e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3572, 'grad_norm': 8.334826469421387, 'learning_rate': 3.35e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2964, 'grad_norm': 8.509514808654785, 'learning_rate': 3.345000000000001e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3346, 'grad_norm': 9.062234878540039, 'learning_rate': 3.3400000000000005e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3599, 'grad_norm': 9.921650886535645, 'learning_rate': 3.3350000000000004e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3121, 'grad_norm': 10.736590385437012, 'learning_rate': 3.33e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3493, 'grad_norm': 8.786526679992676, 'learning_rate': 3.325e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2984, 'grad_norm': 8.863507270812988, 'learning_rate': 3.32e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3447, 'grad_norm': 8.68010425567627, 'learning_rate': 3.3150000000000006e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3758, 'grad_norm': 8.379789352416992, 'learning_rate': 3.3100000000000005e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3542, 'grad_norm': 8.792935371398926, 'learning_rate': 3.3050000000000004e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2845, 'grad_norm': 8.664155006408691, 'learning_rate': 3.3e-05, 'epoch': 0.71}\n",
      " 34%|███████████▏                     | 10200/30000 [2:30:59<4:58:30,  1.11it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:17<00:17,  8.91s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:35<00:12, 12.55s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.829750000000004, 'eval_rouge-2': 7.6466280000000015, 'eval_rouge-l': 25.122282, 'eval_bleu-4': 0.03845538862313367, 'eval_runtime': 55.2151, 'eval_samples_per_second': 0.906, 'eval_steps_per_second': 0.072, 'epoch': 0.71}\n",
      " 34%|███████████▏                     | 10200/30000 [2:31:55<4:58:30,  1.11it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:37<00:00,  8.69s/it]\u001b[A\n",
      "{'loss': 3.3447, 'grad_norm': 8.386808395385742, 'learning_rate': 3.295e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3005, 'grad_norm': 8.761475563049316, 'learning_rate': 3.29e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3053, 'grad_norm': 8.299095153808594, 'learning_rate': 3.2850000000000006e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2876, 'grad_norm': 7.655813694000244, 'learning_rate': 3.2800000000000004e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3003, 'grad_norm': 7.644498348236084, 'learning_rate': 3.275e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3254, 'grad_norm': 8.254083633422852, 'learning_rate': 3.27e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3367, 'grad_norm': 9.934296607971191, 'learning_rate': 3.265e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3125, 'grad_norm': 8.861452102661133, 'learning_rate': 3.26e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3784, 'grad_norm': 7.989501953125, 'learning_rate': 3.2550000000000005e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3973, 'grad_norm': 9.571958541870117, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3374, 'grad_norm': 9.661818504333496, 'learning_rate': 3.245e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3197, 'grad_norm': 8.600859642028809, 'learning_rate': 3.24e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3176, 'grad_norm': 8.140233039855957, 'learning_rate': 3.235e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2981, 'grad_norm': 10.331164360046387, 'learning_rate': 3.2300000000000006e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3023, 'grad_norm': 8.135086059570312, 'learning_rate': 3.2250000000000005e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2971, 'grad_norm': 8.808670043945312, 'learning_rate': 3.2200000000000003e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2805, 'grad_norm': 8.183263778686523, 'learning_rate': 3.215e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3765, 'grad_norm': 8.255475044250488, 'learning_rate': 3.21e-05, 'epoch': 0.75}\n",
      "{'loss': 3.271, 'grad_norm': 8.031144142150879, 'learning_rate': 3.205e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3847, 'grad_norm': 7.899104118347168, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.75}\n",
      " 36%|███████████▉                     | 10800/30000 [2:40:01<4:27:20,  1.20it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.05s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.42s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.9975, 'eval_rouge-2': 7.690348000000001, 'eval_rouge-l': 25.141918, 'eval_bleu-4': 0.03639394555216726, 'eval_runtime': 30.6084, 'eval_samples_per_second': 1.634, 'eval_steps_per_second': 0.131, 'epoch': 0.75}\n",
      " 36%|███████████▉                     | 10800/30000 [2:40:31<4:27:20,  1.20it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.44s/it]\u001b[A\n",
      "{'loss': 3.3111, 'grad_norm': 8.343146324157715, 'learning_rate': 3.1950000000000004e-05, 'epoch': 0.76}\n",
      "{'loss': 3.293, 'grad_norm': 8.35800552368164, 'learning_rate': 3.19e-05, 'epoch': 0.76}\n",
      "{'loss': 3.348, 'grad_norm': 8.924710273742676, 'learning_rate': 3.185e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3238, 'grad_norm': 8.333505630493164, 'learning_rate': 3.18e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3137, 'grad_norm': 9.352397918701172, 'learning_rate': 3.175e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3268, 'grad_norm': 9.075396537780762, 'learning_rate': 3.1700000000000005e-05, 'epoch': 0.77}\n",
      "{'loss': 3.323, 'grad_norm': 8.569669723510742, 'learning_rate': 3.1650000000000004e-05, 'epoch': 0.77}\n",
      "{'loss': 3.3361, 'grad_norm': 8.142847061157227, 'learning_rate': 3.16e-05, 'epoch': 0.77}\n",
      "{'loss': 3.3458, 'grad_norm': 9.061990737915039, 'learning_rate': 3.155e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2956, 'grad_norm': 8.12536334991455, 'learning_rate': 3.15e-05, 'epoch': 0.77}\n",
      "{'loss': 3.3028, 'grad_norm': 9.090669631958008, 'learning_rate': 3.145e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3587, 'grad_norm': 7.986002445220947, 'learning_rate': 3.1400000000000004e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3186, 'grad_norm': 8.990300178527832, 'learning_rate': 3.135e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3791, 'grad_norm': 9.703433990478516, 'learning_rate': 3.13e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2756, 'grad_norm': 9.413392066955566, 'learning_rate': 3.125e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2811, 'grad_norm': 8.166502952575684, 'learning_rate': 3.12e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2692, 'grad_norm': 8.785185813903809, 'learning_rate': 3.115e-05, 'epoch': 0.79}\n",
      "{'loss': 3.292, 'grad_norm': 11.406595230102539, 'learning_rate': 3.1100000000000004e-05, 'epoch': 0.79}\n",
      "{'loss': 3.3464, 'grad_norm': 9.05135440826416, 'learning_rate': 3.105e-05, 'epoch': 0.79}\n",
      "{'loss': 3.3539, 'grad_norm': 8.567182540893555, 'learning_rate': 3.1e-05, 'epoch': 0.8}\n",
      " 38%|████████████▌                    | 11400/30000 [2:48:33<4:09:57,  1.24it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.64s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:25<00:09,  9.97s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.00663599999999, 'eval_rouge-2': 7.2333919999999985, 'eval_rouge-l': 25.120162, 'eval_bleu-4': 0.03783546407366316, 'eval_runtime': 34.8372, 'eval_samples_per_second': 1.435, 'eval_steps_per_second': 0.115, 'epoch': 0.8}\n",
      " 38%|████████████▌                    | 11400/30000 [2:49:08<4:09:57,  1.24it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:28<00:00,  7.37s/it]\u001b[A\n",
      "{'loss': 3.3046, 'grad_norm': 8.591577529907227, 'learning_rate': 3.095e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2696, 'grad_norm': 8.424235343933105, 'learning_rate': 3.09e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2982, 'grad_norm': 8.394783973693848, 'learning_rate': 3.0850000000000004e-05, 'epoch': 0.8}\n",
      "{'loss': 3.3286, 'grad_norm': 8.331716537475586, 'learning_rate': 3.08e-05, 'epoch': 0.8}\n",
      "{'loss': 3.3372, 'grad_norm': 8.983453750610352, 'learning_rate': 3.075e-05, 'epoch': 0.81}\n",
      "{'loss': 3.3442, 'grad_norm': 9.550460815429688, 'learning_rate': 3.07e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2683, 'grad_norm': 8.758109092712402, 'learning_rate': 3.065e-05, 'epoch': 0.81}\n",
      "{'loss': 3.3385, 'grad_norm': 8.569275856018066, 'learning_rate': 3.06e-05, 'epoch': 0.81}\n",
      "{'loss': 3.3196, 'grad_norm': 8.32867431640625, 'learning_rate': 3.0550000000000004e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2983, 'grad_norm': 9.145594596862793, 'learning_rate': 3.05e-05, 'epoch': 0.82}\n",
      "{'loss': 3.3478, 'grad_norm': 8.948760032653809, 'learning_rate': 3.045e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2851, 'grad_norm': 8.852095603942871, 'learning_rate': 3.04e-05, 'epoch': 0.82}\n",
      "{'loss': 3.335, 'grad_norm': 7.768430233001709, 'learning_rate': 3.035e-05, 'epoch': 0.82}\n",
      "{'loss': 3.3224, 'grad_norm': 9.148200035095215, 'learning_rate': 3.03e-05, 'epoch': 0.83}\n",
      "{'loss': 3.3239, 'grad_norm': 8.465315818786621, 'learning_rate': 3.025e-05, 'epoch': 0.83}\n",
      "{'loss': 3.3477, 'grad_norm': 9.22649097442627, 'learning_rate': 3.02e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2992, 'grad_norm': 8.847088813781738, 'learning_rate': 3.015e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2855, 'grad_norm': 8.30440902709961, 'learning_rate': 3.01e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2848, 'grad_norm': 8.29787540435791, 'learning_rate': 3.0050000000000002e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2961, 'grad_norm': 9.219470024108887, 'learning_rate': 3e-05, 'epoch': 0.84}\n",
      " 40%|█████████████▏                   | 12000/30000 [2:57:12<4:13:40,  1.18it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.02s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:09<00:03,  3.52s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.277322000000005, 'eval_rouge-2': 7.850884000000001, 'eval_rouge-l': 25.397574, 'eval_bleu-4': 0.037416939914361506, 'eval_runtime': 16.3083, 'eval_samples_per_second': 3.066, 'eval_steps_per_second': 0.245, 'epoch': 0.84}\n",
      " 40%|█████████████▏                   | 12000/30000 [2:57:29<4:13:40,  1.18it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:13<00:00,  3.40s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-12000\n",
      "tokenizer config file saved in ./output/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 3.2924, 'grad_norm': 9.37556266784668, 'learning_rate': 2.995e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2938, 'grad_norm': 7.983429431915283, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.84}\n",
      "{'loss': 3.265, 'grad_norm': 8.933476448059082, 'learning_rate': 2.985e-05, 'epoch': 0.84}\n",
      "{'loss': 3.3427, 'grad_norm': 8.628329277038574, 'learning_rate': 2.98e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3637, 'grad_norm': 8.521868705749512, 'learning_rate': 2.975e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3007, 'grad_norm': 8.745330810546875, 'learning_rate': 2.97e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2811, 'grad_norm': 8.958880424499512, 'learning_rate': 2.965e-05, 'epoch': 0.85}\n",
      "{'loss': 3.302, 'grad_norm': 8.209521293640137, 'learning_rate': 2.96e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3113, 'grad_norm': 8.313124656677246, 'learning_rate': 2.955e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2699, 'grad_norm': 9.011054039001465, 'learning_rate': 2.95e-05, 'epoch': 0.86}\n",
      "{'loss': 3.3221, 'grad_norm': 8.036526679992676, 'learning_rate': 2.945e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2767, 'grad_norm': 8.75328254699707, 'learning_rate': 2.94e-05, 'epoch': 0.86}\n",
      "{'loss': 3.3251, 'grad_norm': 8.827677726745605, 'learning_rate': 2.935e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2991, 'grad_norm': 8.68990707397461, 'learning_rate': 2.93e-05, 'epoch': 0.87}\n",
      "{'loss': 3.324, 'grad_norm': 8.93113899230957, 'learning_rate': 2.925e-05, 'epoch': 0.87}\n",
      "{'loss': 3.3029, 'grad_norm': 9.275419235229492, 'learning_rate': 2.9199999999999998e-05, 'epoch': 0.87}\n",
      "{'loss': 3.3145, 'grad_norm': 9.348833084106445, 'learning_rate': 2.915e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2724, 'grad_norm': 9.595924377441406, 'learning_rate': 2.91e-05, 'epoch': 0.88}\n",
      "{'loss': 3.3088, 'grad_norm': 8.603163719177246, 'learning_rate': 2.9049999999999998e-05, 'epoch': 0.88}\n",
      "{'loss': 3.274, 'grad_norm': 10.582727432250977, 'learning_rate': 2.9e-05, 'epoch': 0.88}\n",
      " 42%|█████████████▊                   | 12600/30000 [3:05:34<4:00:32,  1.21it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.12s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:12<00:04,  4.84s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.51716, 'eval_rouge-2': 7.886593999999999, 'eval_rouge-l': 25.554777999999995, 'eval_bleu-4': 0.03748845370440148, 'eval_runtime': 20.3538, 'eval_samples_per_second': 2.457, 'eval_steps_per_second': 0.197, 'epoch': 0.88}\n",
      " 42%|█████████████▊                   | 12600/30000 [3:05:55<4:00:32,  1.21it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.82s/it]\u001b[A\n",
      "{'loss': 3.2973, 'grad_norm': 9.25377082824707, 'learning_rate': 2.895e-05, 'epoch': 0.88}\n",
      "{'loss': 3.3277, 'grad_norm': 10.221545219421387, 'learning_rate': 2.8899999999999998e-05, 'epoch': 0.88}\n",
      "{'loss': 3.299, 'grad_norm': 9.72429084777832, 'learning_rate': 2.885e-05, 'epoch': 0.89}\n",
      "{'loss': 3.3572, 'grad_norm': 8.738418579101562, 'learning_rate': 2.88e-05, 'epoch': 0.89}\n",
      "{'loss': 3.3063, 'grad_norm': 8.854873657226562, 'learning_rate': 2.8749999999999997e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2747, 'grad_norm': 10.750922203063965, 'learning_rate': 2.87e-05, 'epoch': 0.89}\n",
      "{'loss': 3.3354, 'grad_norm': 9.39552116394043, 'learning_rate': 2.865e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2792, 'grad_norm': 9.017908096313477, 'learning_rate': 2.86e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3197, 'grad_norm': 9.0848388671875, 'learning_rate': 2.855e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2971, 'grad_norm': 8.780381202697754, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3453, 'grad_norm': 9.00407886505127, 'learning_rate': 2.845e-05, 'epoch': 0.9}\n",
      "{'loss': 3.3166, 'grad_norm': 8.75960922241211, 'learning_rate': 2.84e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2753, 'grad_norm': 9.540194511413574, 'learning_rate': 2.8349999999999998e-05, 'epoch': 0.91}\n",
      "{'loss': 3.3139, 'grad_norm': 8.73586654663086, 'learning_rate': 2.83e-05, 'epoch': 0.91}\n",
      "{'loss': 3.251, 'grad_norm': 8.833396911621094, 'learning_rate': 2.825e-05, 'epoch': 0.91}\n",
      "{'loss': 3.3624, 'grad_norm': 9.186555862426758, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.91}\n",
      "{'loss': 3.3228, 'grad_norm': 14.058568954467773, 'learning_rate': 2.815e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2922, 'grad_norm': 9.253013610839844, 'learning_rate': 2.8100000000000005e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3349, 'grad_norm': 9.128667831420898, 'learning_rate': 2.8050000000000004e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3185, 'grad_norm': 9.116586685180664, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.92}\n",
      " 44%|██████████████▌                  | 13200/30000 [3:13:58<3:42:38,  1.26it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.11s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.53s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.711743999999996, 'eval_rouge-2': 8.896588, 'eval_rouge-l': 25.136416, 'eval_bleu-4': 0.04173428928411599, 'eval_runtime': 31.8883, 'eval_samples_per_second': 1.568, 'eval_steps_per_second': 0.125, 'epoch': 0.92}\n",
      " 44%|██████████████▌                  | 13200/30000 [3:14:30<3:42:38,  1.26it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.61s/it]\u001b[A\n",
      "{'loss': 3.3506, 'grad_norm': 11.270685195922852, 'learning_rate': 2.7950000000000005e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3242, 'grad_norm': 8.987603187561035, 'learning_rate': 2.7900000000000004e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2994, 'grad_norm': 8.314291000366211, 'learning_rate': 2.7850000000000003e-05, 'epoch': 0.93}\n",
      "{'loss': 3.3327, 'grad_norm': 8.171869277954102, 'learning_rate': 2.7800000000000005e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2965, 'grad_norm': 8.76047420501709, 'learning_rate': 2.7750000000000004e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2962, 'grad_norm': 9.679293632507324, 'learning_rate': 2.7700000000000002e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2712, 'grad_norm': 9.242923736572266, 'learning_rate': 2.7650000000000005e-05, 'epoch': 0.94}\n",
      "{'loss': 3.265, 'grad_norm': 9.049314498901367, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3256, 'grad_norm': 9.539715766906738, 'learning_rate': 2.7550000000000002e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3101, 'grad_norm': 10.191028594970703, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.94}\n",
      "{'loss': 3.3067, 'grad_norm': 9.527758598327637, 'learning_rate': 2.7450000000000003e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2675, 'grad_norm': 8.431251525878906, 'learning_rate': 2.7400000000000002e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2932, 'grad_norm': 9.11157512664795, 'learning_rate': 2.7350000000000004e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2867, 'grad_norm': 10.902789115905762, 'learning_rate': 2.7300000000000003e-05, 'epoch': 0.95}\n",
      "{'loss': 3.272, 'grad_norm': 9.224430084228516, 'learning_rate': 2.725e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2382, 'grad_norm': 8.955729484558105, 'learning_rate': 2.7200000000000004e-05, 'epoch': 0.95}\n",
      "{'loss': 3.3425, 'grad_norm': 9.519970893859863, 'learning_rate': 2.7150000000000003e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2953, 'grad_norm': 9.503257751464844, 'learning_rate': 2.7100000000000005e-05, 'epoch': 0.96}\n",
      "{'loss': 3.3275, 'grad_norm': 9.72795295715332, 'learning_rate': 2.7050000000000004e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2751, 'grad_norm': 10.134040832519531, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.96}\n",
      " 46%|███████████████▏                 | 13800/30000 [3:22:32<3:20:50,  1.34it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.79s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:26<00:10, 10.46s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.644686, 'eval_rouge-2': 7.288241999999999, 'eval_rouge-l': 24.884276, 'eval_bleu-4': 0.03513597088971106, 'eval_runtime': 51.1678, 'eval_samples_per_second': 0.977, 'eval_steps_per_second': 0.078, 'epoch': 0.96}\n",
      " 46%|███████████████▏                 | 13800/30000 [3:23:23<3:20:50,  1.34it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:29<00:00,  7.50s/it]\u001b[A\n",
      "{'loss': 3.282, 'grad_norm': 8.47372055053711, 'learning_rate': 2.6950000000000005e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2951, 'grad_norm': 9.395872116088867, 'learning_rate': 2.6900000000000003e-05, 'epoch': 0.97}\n",
      "{'loss': 3.291, 'grad_norm': 9.13900089263916, 'learning_rate': 2.6850000000000002e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2279, 'grad_norm': 8.917143821716309, 'learning_rate': 2.6800000000000004e-05, 'epoch': 0.97}\n",
      "{'loss': 3.3096, 'grad_norm': 8.724886894226074, 'learning_rate': 2.6750000000000003e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2589, 'grad_norm': 9.130744934082031, 'learning_rate': 2.6700000000000002e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2313, 'grad_norm': 10.02310562133789, 'learning_rate': 2.6650000000000004e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2815, 'grad_norm': 9.221684455871582, 'learning_rate': 2.6600000000000003e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2847, 'grad_norm': 9.539568901062012, 'learning_rate': 2.655e-05, 'epoch': 0.98}\n",
      "{'loss': 3.3312, 'grad_norm': 9.161697387695312, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2846, 'grad_norm': 9.736571311950684, 'learning_rate': 2.6450000000000003e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2889, 'grad_norm': 10.551782608032227, 'learning_rate': 2.64e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2727, 'grad_norm': 9.773680686950684, 'learning_rate': 2.6350000000000004e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3012, 'grad_norm': 8.966436386108398, 'learning_rate': 2.6300000000000002e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3211, 'grad_norm': 9.177885055541992, 'learning_rate': 2.625e-05, 'epoch': 0.99}\n",
      "{'loss': 3.3224, 'grad_norm': 8.966737747192383, 'learning_rate': 2.6200000000000003e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2515, 'grad_norm': 9.163863182067871, 'learning_rate': 2.6150000000000002e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2449, 'grad_norm': 8.918720245361328, 'learning_rate': 2.61e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2517, 'grad_norm': 9.183629035949707, 'learning_rate': 2.6050000000000003e-05, 'epoch': 1.0}\n",
      "{'loss': 3.3077, 'grad_norm': 9.361655235290527, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.01}\n",
      " 48%|███████████████▊                 | 14400/30000 [3:31:27<3:30:07,  1.24it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.10s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:03,  3.16s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.62236, 'eval_rouge-2': 8.398276, 'eval_rouge-l': 25.832049999999995, 'eval_bleu-4': 0.03660523856537297, 'eval_runtime': 14.3479, 'eval_samples_per_second': 3.485, 'eval_steps_per_second': 0.279, 'epoch': 1.01}\n",
      " 48%|███████████████▊                 | 14400/30000 [3:31:42<3:30:07,  1.24it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.69s/it]\u001b[A\n",
      "{'loss': 3.2693, 'grad_norm': 9.252644538879395, 'learning_rate': 2.595e-05, 'epoch': 1.01}\n",
      "{'loss': 3.243, 'grad_norm': 8.764666557312012, 'learning_rate': 2.5900000000000003e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2183, 'grad_norm': 8.701881408691406, 'learning_rate': 2.585e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2462, 'grad_norm': 8.630572319030762, 'learning_rate': 2.58e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2868, 'grad_norm': 8.772416114807129, 'learning_rate': 2.5750000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2658, 'grad_norm': 8.145333290100098, 'learning_rate': 2.57e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2682, 'grad_norm': 8.812600135803223, 'learning_rate': 2.5650000000000003e-05, 'epoch': 1.02}\n",
      "{'loss': 3.22, 'grad_norm': 9.256241798400879, 'learning_rate': 2.5600000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2229, 'grad_norm': 8.900362968444824, 'learning_rate': 2.555e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2649, 'grad_norm': 9.411721229553223, 'learning_rate': 2.5500000000000003e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2814, 'grad_norm': 9.311565399169922, 'learning_rate': 2.5450000000000002e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2676, 'grad_norm': 9.229836463928223, 'learning_rate': 2.54e-05, 'epoch': 1.03}\n",
      "{'loss': 3.3045, 'grad_norm': 9.005926132202148, 'learning_rate': 2.5350000000000003e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2297, 'grad_norm': 10.427332878112793, 'learning_rate': 2.5300000000000002e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2277, 'grad_norm': 9.700202941894531, 'learning_rate': 2.525e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2779, 'grad_norm': 9.30628776550293, 'learning_rate': 2.5200000000000003e-05, 'epoch': 1.04}\n",
      "{'loss': 3.3228, 'grad_norm': 9.889424324035645, 'learning_rate': 2.515e-05, 'epoch': 1.04}\n",
      "{'loss': 3.237, 'grad_norm': 10.218441009521484, 'learning_rate': 2.51e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2605, 'grad_norm': 10.17640495300293, 'learning_rate': 2.5050000000000002e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2516, 'grad_norm': 9.332700729370117, 'learning_rate': 2.5e-05, 'epoch': 1.05}\n",
      " 50%|████████████████▌                | 15000/30000 [3:39:49<3:19:47,  1.25it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.38s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.74s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.546088, 'eval_rouge-2': 8.350157999999999, 'eval_rouge-l': 24.646528, 'eval_bleu-4': 0.041317032847575784, 'eval_runtime': 31.1353, 'eval_samples_per_second': 1.606, 'eval_steps_per_second': 0.128, 'epoch': 1.05}\n",
      " 50%|████████████████▌                | 15000/30000 [3:40:20<3:19:47,  1.25it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.50s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-15000\n",
      "tokenizer config file saved in ./output/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-15000/special_tokens_map.json\n",
      "{'loss': 3.2824, 'grad_norm': 9.504913330078125, 'learning_rate': 2.495e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2319, 'grad_norm': 9.374537467956543, 'learning_rate': 2.4900000000000002e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2626, 'grad_norm': 9.676497459411621, 'learning_rate': 2.485e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2717, 'grad_norm': 9.30184555053711, 'learning_rate': 2.48e-05, 'epoch': 1.06}\n",
      "{'loss': 3.3233, 'grad_norm': 9.782358169555664, 'learning_rate': 2.4750000000000002e-05, 'epoch': 1.06}\n",
      "{'loss': 3.3268, 'grad_norm': 10.279586791992188, 'learning_rate': 2.47e-05, 'epoch': 1.06}\n",
      "{'loss': 3.207, 'grad_norm': 8.57774829864502, 'learning_rate': 2.465e-05, 'epoch': 1.06}\n",
      "{'loss': 3.2989, 'grad_norm': 8.557100296020508, 'learning_rate': 2.46e-05, 'epoch': 1.06}\n",
      "{'loss': 3.2984, 'grad_norm': 8.975163459777832, 'learning_rate': 2.455e-05, 'epoch': 1.07}\n",
      "{'loss': 3.282, 'grad_norm': 9.217330932617188, 'learning_rate': 2.45e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2264, 'grad_norm': 9.657617568969727, 'learning_rate': 2.445e-05, 'epoch': 1.07}\n",
      "{'loss': 3.269, 'grad_norm': 8.953042030334473, 'learning_rate': 2.44e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2798, 'grad_norm': 9.030228614807129, 'learning_rate': 2.435e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2529, 'grad_norm': 8.833988189697266, 'learning_rate': 2.43e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2601, 'grad_norm': 10.095698356628418, 'learning_rate': 2.425e-05, 'epoch': 1.08}\n",
      "{'loss': 3.3003, 'grad_norm': 9.332891464233398, 'learning_rate': 2.4200000000000002e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2421, 'grad_norm': 10.708588600158691, 'learning_rate': 2.415e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2055, 'grad_norm': 9.333166122436523, 'learning_rate': 2.41e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2572, 'grad_norm': 9.71950626373291, 'learning_rate': 2.4050000000000002e-05, 'epoch': 1.09}\n",
      "{'loss': 3.263, 'grad_norm': 9.941402435302734, 'learning_rate': 2.4e-05, 'epoch': 1.09}\n",
      " 52%|█████████████████▏               | 15600/30000 [3:48:26<3:12:00,  1.25it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.72s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.35s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.278916, 'eval_rouge-2': 7.720804, 'eval_rouge-l': 25.869672, 'eval_bleu-4': 0.03641698028678308, 'eval_runtime': 15.693, 'eval_samples_per_second': 3.186, 'eval_steps_per_second': 0.255, 'epoch': 1.09}\n",
      " 52%|█████████████████▏               | 15600/30000 [3:48:42<3:12:00,  1.25it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.72s/it]\u001b[A\n",
      "{'loss': 3.2923, 'grad_norm': 9.234856605529785, 'learning_rate': 2.395e-05, 'epoch': 1.09}\n",
      "{'loss': 3.274, 'grad_norm': 9.86043930053711, 'learning_rate': 2.39e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2952, 'grad_norm': 9.93963623046875, 'learning_rate': 2.385e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2894, 'grad_norm': 8.884995460510254, 'learning_rate': 2.38e-05, 'epoch': 1.1}\n",
      "{'loss': 3.293, 'grad_norm': 9.148579597473145, 'learning_rate': 2.375e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2307, 'grad_norm': 9.295178413391113, 'learning_rate': 2.37e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2524, 'grad_norm': 10.055519104003906, 'learning_rate': 2.365e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2314, 'grad_norm': 10.193236351013184, 'learning_rate': 2.36e-05, 'epoch': 1.11}\n",
      "{'loss': 3.284, 'grad_norm': 8.674560546875, 'learning_rate': 2.355e-05, 'epoch': 1.11}\n",
      "{'loss': 3.3175, 'grad_norm': 9.100682258605957, 'learning_rate': 2.35e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2818, 'grad_norm': 8.497400283813477, 'learning_rate': 2.345e-05, 'epoch': 1.11}\n",
      "{'loss': 3.3, 'grad_norm': 9.998147010803223, 'learning_rate': 2.3400000000000003e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2457, 'grad_norm': 10.087103843688965, 'learning_rate': 2.3350000000000002e-05, 'epoch': 1.12}\n",
      "{'loss': 3.2622, 'grad_norm': 8.863574028015137, 'learning_rate': 2.3300000000000004e-05, 'epoch': 1.12}\n",
      "{'loss': 3.2566, 'grad_norm': 10.120811462402344, 'learning_rate': 2.3250000000000003e-05, 'epoch': 1.12}\n",
      "{'loss': 3.2498, 'grad_norm': 10.526460647583008, 'learning_rate': 2.32e-05, 'epoch': 1.12}\n",
      "{'loss': 3.2874, 'grad_norm': 8.848041534423828, 'learning_rate': 2.3150000000000004e-05, 'epoch': 1.12}\n",
      "{'loss': 3.3509, 'grad_norm': 8.840935707092285, 'learning_rate': 2.3100000000000002e-05, 'epoch': 1.13}\n",
      "{'loss': 3.2777, 'grad_norm': 9.195535659790039, 'learning_rate': 2.305e-05, 'epoch': 1.13}\n",
      "{'loss': 3.2593, 'grad_norm': 10.012346267700195, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.13}\n",
      " 54%|█████████████████▊               | 16200/30000 [3:56:48<3:16:27,  1.17it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.99s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:11<00:04,  4.47s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.580588, 'eval_rouge-2': 7.971514, 'eval_rouge-l': 26.276934, 'eval_bleu-4': 0.04020386272555692, 'eval_runtime': 18.3354, 'eval_samples_per_second': 2.727, 'eval_steps_per_second': 0.218, 'epoch': 1.13}\n",
      " 54%|█████████████████▊               | 16200/30000 [3:57:06<3:16:27,  1.17it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  3.66s/it]\u001b[A\n",
      "{'loss': 3.2627, 'grad_norm': 10.278360366821289, 'learning_rate': 2.2950000000000002e-05, 'epoch': 1.13}\n",
      "{'loss': 3.2421, 'grad_norm': 9.035737037658691, 'learning_rate': 2.29e-05, 'epoch': 1.14}\n",
      "{'loss': 3.3205, 'grad_norm': 9.800910949707031, 'learning_rate': 2.2850000000000003e-05, 'epoch': 1.14}\n",
      "{'loss': 3.2482, 'grad_norm': 9.359231948852539, 'learning_rate': 2.2800000000000002e-05, 'epoch': 1.14}\n",
      "{'loss': 3.3059, 'grad_norm': 9.51125431060791, 'learning_rate': 2.275e-05, 'epoch': 1.14}\n",
      "{'loss': 3.238, 'grad_norm': 10.597576141357422, 'learning_rate': 2.2700000000000003e-05, 'epoch': 1.14}\n",
      "{'loss': 3.2937, 'grad_norm': 9.585301399230957, 'learning_rate': 2.265e-05, 'epoch': 1.15}\n",
      "{'loss': 3.2758, 'grad_norm': 9.417481422424316, 'learning_rate': 2.26e-05, 'epoch': 1.15}\n",
      "{'loss': 3.2584, 'grad_norm': 9.774337768554688, 'learning_rate': 2.2550000000000003e-05, 'epoch': 1.15}\n",
      "{'loss': 3.2878, 'grad_norm': 9.838258743286133, 'learning_rate': 2.25e-05, 'epoch': 1.15}\n",
      "{'loss': 3.2559, 'grad_norm': 9.560173988342285, 'learning_rate': 2.245e-05, 'epoch': 1.15}\n",
      "{'loss': 3.2588, 'grad_norm': 9.35508918762207, 'learning_rate': 2.2400000000000002e-05, 'epoch': 1.16}\n",
      "{'loss': 3.2728, 'grad_norm': 9.78510856628418, 'learning_rate': 2.235e-05, 'epoch': 1.16}\n",
      "{'loss': 3.3279, 'grad_norm': 10.24548625946045, 'learning_rate': 2.23e-05, 'epoch': 1.16}\n",
      "{'loss': 3.2368, 'grad_norm': 9.263147354125977, 'learning_rate': 2.2250000000000002e-05, 'epoch': 1.16}\n",
      "{'loss': 3.3256, 'grad_norm': 9.459202766418457, 'learning_rate': 2.22e-05, 'epoch': 1.16}\n",
      "{'loss': 3.2952, 'grad_norm': 9.36417293548584, 'learning_rate': 2.215e-05, 'epoch': 1.17}\n",
      "{'loss': 3.3204, 'grad_norm': 11.026205062866211, 'learning_rate': 2.2100000000000002e-05, 'epoch': 1.17}\n",
      "{'loss': 3.3063, 'grad_norm': 10.042375564575195, 'learning_rate': 2.205e-05, 'epoch': 1.17}\n",
      "{'loss': 3.218, 'grad_norm': 9.16933536529541, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.17}\n",
      " 56%|██████████████████▍              | 16800/30000 [4:05:08<2:53:52,  1.27it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.54s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.844568000000002, 'eval_rouge-2': 7.359929999999999, 'eval_rouge-l': 25.199265999999998, 'eval_bleu-4': 0.03603285754761809, 'eval_runtime': 17.4233, 'eval_samples_per_second': 2.87, 'eval_steps_per_second': 0.23, 'epoch': 1.17}\n",
      " 56%|██████████████████▍              | 16800/30000 [4:05:26<2:53:52,  1.27it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:12<00:00,  3.25s/it]\u001b[A\n",
      "{'loss': 3.3003, 'grad_norm': 9.258511543273926, 'learning_rate': 2.195e-05, 'epoch': 1.17}\n",
      "{'loss': 3.2611, 'grad_norm': 9.301191329956055, 'learning_rate': 2.19e-05, 'epoch': 1.18}\n",
      "{'loss': 3.2582, 'grad_norm': 9.319873809814453, 'learning_rate': 2.1850000000000003e-05, 'epoch': 1.18}\n",
      "{'loss': 3.2624, 'grad_norm': 9.910027503967285, 'learning_rate': 2.18e-05, 'epoch': 1.18}\n",
      "{'loss': 3.2951, 'grad_norm': 9.48111343383789, 'learning_rate': 2.175e-05, 'epoch': 1.18}\n",
      "{'loss': 3.2853, 'grad_norm': 9.730179786682129, 'learning_rate': 2.1700000000000002e-05, 'epoch': 1.19}\n",
      "{'loss': 3.2047, 'grad_norm': 9.072566986083984, 'learning_rate': 2.165e-05, 'epoch': 1.19}\n",
      "{'loss': 3.254, 'grad_norm': 9.836713790893555, 'learning_rate': 2.16e-05, 'epoch': 1.19}\n",
      "{'loss': 3.3123, 'grad_norm': 10.311561584472656, 'learning_rate': 2.1550000000000002e-05, 'epoch': 1.19}\n",
      "{'loss': 3.2905, 'grad_norm': 9.902706146240234, 'learning_rate': 2.15e-05, 'epoch': 1.19}\n",
      "{'loss': 3.2632, 'grad_norm': 10.255874633789062, 'learning_rate': 2.145e-05, 'epoch': 1.2}\n",
      "{'loss': 3.3188, 'grad_norm': 10.87154483795166, 'learning_rate': 2.1400000000000002e-05, 'epoch': 1.2}\n",
      "{'loss': 3.3163, 'grad_norm': 9.175710678100586, 'learning_rate': 2.135e-05, 'epoch': 1.2}\n",
      "{'loss': 3.2561, 'grad_norm': 9.301704406738281, 'learning_rate': 2.13e-05, 'epoch': 1.2}\n",
      "{'loss': 3.2233, 'grad_norm': 9.761062622070312, 'learning_rate': 2.125e-05, 'epoch': 1.2}\n",
      "{'loss': 3.3036, 'grad_norm': 9.17308235168457, 'learning_rate': 2.12e-05, 'epoch': 1.21}\n",
      "{'loss': 3.281, 'grad_norm': 10.326375961303711, 'learning_rate': 2.115e-05, 'epoch': 1.21}\n",
      "{'loss': 3.2823, 'grad_norm': 9.835917472839355, 'learning_rate': 2.11e-05, 'epoch': 1.21}\n",
      "{'loss': 3.2678, 'grad_norm': 9.652353286743164, 'learning_rate': 2.105e-05, 'epoch': 1.21}\n",
      "{'loss': 3.3132, 'grad_norm': 11.442931175231934, 'learning_rate': 2.1e-05, 'epoch': 1.21}\n",
      " 58%|███████████████████▏             | 17400/30000 [4:13:31<2:45:59,  1.27it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.67s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.91s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.900776, 'eval_rouge-2': 8.505962, 'eval_rouge-l': 25.819446, 'eval_bleu-4': 0.0382503554777976, 'eval_runtime': 31.8096, 'eval_samples_per_second': 1.572, 'eval_steps_per_second': 0.126, 'epoch': 1.21}\n",
      " 58%|███████████████████▏             | 17400/30000 [4:14:02<2:45:59,  1.27it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:11<00:00,  2.69s/it]\u001b[A\n",
      "{'loss': 3.3181, 'grad_norm': 9.412691116333008, 'learning_rate': 2.095e-05, 'epoch': 1.22}\n",
      "{'loss': 3.293, 'grad_norm': 11.150474548339844, 'learning_rate': 2.09e-05, 'epoch': 1.22}\n",
      "{'loss': 3.2326, 'grad_norm': 10.503575325012207, 'learning_rate': 2.085e-05, 'epoch': 1.22}\n",
      "{'loss': 3.264, 'grad_norm': 9.899950981140137, 'learning_rate': 2.08e-05, 'epoch': 1.22}\n",
      "{'loss': 3.2871, 'grad_norm': 10.389764785766602, 'learning_rate': 2.075e-05, 'epoch': 1.23}\n",
      "{'loss': 3.2486, 'grad_norm': 11.47249698638916, 'learning_rate': 2.07e-05, 'epoch': 1.23}\n",
      "{'loss': 3.3012, 'grad_norm': 11.740412712097168, 'learning_rate': 2.065e-05, 'epoch': 1.23}\n",
      "{'loss': 3.2533, 'grad_norm': 9.624035835266113, 'learning_rate': 2.06e-05, 'epoch': 1.23}\n",
      "{'loss': 3.2714, 'grad_norm': 10.355636596679688, 'learning_rate': 2.055e-05, 'epoch': 1.23}\n",
      "{'loss': 3.266, 'grad_norm': 9.84151840209961, 'learning_rate': 2.05e-05, 'epoch': 1.24}\n",
      "{'loss': 3.3293, 'grad_norm': 8.90761661529541, 'learning_rate': 2.045e-05, 'epoch': 1.24}\n",
      "{'loss': 3.3014, 'grad_norm': 10.579593658447266, 'learning_rate': 2.04e-05, 'epoch': 1.24}\n",
      "{'loss': 3.2742, 'grad_norm': 10.064453125, 'learning_rate': 2.035e-05, 'epoch': 1.24}\n",
      "{'loss': 3.2118, 'grad_norm': 10.120330810546875, 'learning_rate': 2.0300000000000002e-05, 'epoch': 1.24}\n",
      "{'loss': 3.2133, 'grad_norm': 9.09634017944336, 'learning_rate': 2.025e-05, 'epoch': 1.25}\n",
      "{'loss': 3.2678, 'grad_norm': 10.201778411865234, 'learning_rate': 2.0200000000000003e-05, 'epoch': 1.25}\n",
      "{'loss': 3.2943, 'grad_norm': 10.322787284851074, 'learning_rate': 2.0150000000000002e-05, 'epoch': 1.25}\n",
      "{'loss': 3.2497, 'grad_norm': 10.984990119934082, 'learning_rate': 2.01e-05, 'epoch': 1.25}\n",
      "{'loss': 3.2307, 'grad_norm': 9.715959548950195, 'learning_rate': 2.0050000000000003e-05, 'epoch': 1.25}\n",
      "{'loss': 3.2439, 'grad_norm': 9.428027153015137, 'learning_rate': 2e-05, 'epoch': 1.26}\n",
      " 60%|███████████████████▊             | 18000/30000 [4:22:06<2:45:50,  1.21it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.19s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.71s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.249984, 'eval_rouge-2': 8.798388, 'eval_rouge-l': 26.024430000000002, 'eval_bleu-4': 0.04348490114652509, 'eval_runtime': 14.2512, 'eval_samples_per_second': 3.508, 'eval_steps_per_second': 0.281, 'epoch': 1.26}\n",
      " 60%|███████████████████▊             | 18000/30000 [4:22:20<2:45:50,  1.21it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.65s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-18000\n",
      "tokenizer config file saved in ./output/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-18000/special_tokens_map.json\n",
      "{'loss': 3.2855, 'grad_norm': 12.312085151672363, 'learning_rate': 1.995e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2275, 'grad_norm': 10.987083435058594, 'learning_rate': 1.9900000000000003e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2994, 'grad_norm': 10.68403434753418, 'learning_rate': 1.985e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2398, 'grad_norm': 9.46814250946045, 'learning_rate': 1.9800000000000004e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2856, 'grad_norm': 9.859878540039062, 'learning_rate': 1.9750000000000002e-05, 'epoch': 1.27}\n",
      "{'loss': 3.3228, 'grad_norm': 9.89972972869873, 'learning_rate': 1.97e-05, 'epoch': 1.27}\n",
      "{'loss': 3.229, 'grad_norm': 10.000931739807129, 'learning_rate': 1.9650000000000003e-05, 'epoch': 1.27}\n",
      "{'loss': 3.268, 'grad_norm': 9.137825965881348, 'learning_rate': 1.9600000000000002e-05, 'epoch': 1.27}\n",
      "{'loss': 3.2581, 'grad_norm': 10.666146278381348, 'learning_rate': 1.955e-05, 'epoch': 1.28}\n",
      "{'loss': 3.2443, 'grad_norm': 9.941977500915527, 'learning_rate': 1.9500000000000003e-05, 'epoch': 1.28}\n",
      "{'loss': 3.3049, 'grad_norm': 9.604239463806152, 'learning_rate': 1.9450000000000002e-05, 'epoch': 1.28}\n",
      "{'loss': 3.2661, 'grad_norm': 10.365354537963867, 'learning_rate': 1.94e-05, 'epoch': 1.28}\n",
      "{'loss': 3.3033, 'grad_norm': 9.613306999206543, 'learning_rate': 1.9350000000000003e-05, 'epoch': 1.28}\n",
      "{'loss': 3.2045, 'grad_norm': 9.208168029785156, 'learning_rate': 1.93e-05, 'epoch': 1.29}\n",
      "{'loss': 3.2475, 'grad_norm': 9.600373268127441, 'learning_rate': 1.925e-05, 'epoch': 1.29}\n",
      "{'loss': 3.2296, 'grad_norm': 11.264167785644531, 'learning_rate': 1.9200000000000003e-05, 'epoch': 1.29}\n",
      "{'loss': 3.2093, 'grad_norm': 13.492722511291504, 'learning_rate': 1.915e-05, 'epoch': 1.29}\n",
      "{'loss': 3.2831, 'grad_norm': 9.735879898071289, 'learning_rate': 1.91e-05, 'epoch': 1.29}\n",
      "{'loss': 3.2436, 'grad_norm': 9.060453414916992, 'learning_rate': 1.9050000000000002e-05, 'epoch': 1.3}\n",
      "{'loss': 3.2404, 'grad_norm': 9.662862777709961, 'learning_rate': 1.9e-05, 'epoch': 1.3}\n",
      " 62%|████████████████████▍            | 18600/30000 [4:30:22<2:37:10,  1.21it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:07<00:07,  3.90s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:11<00:03,  3.78s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.590706, 'eval_rouge-2': 8.870562000000001, 'eval_rouge-l': 26.217502, 'eval_bleu-4': 0.04253155593204676, 'eval_runtime': 19.4381, 'eval_samples_per_second': 2.572, 'eval_steps_per_second': 0.206, 'epoch': 1.3}\n",
      " 62%|████████████████████▍            | 18600/30000 [4:30:42<2:37:10,  1.21it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.94s/it]\u001b[A\n",
      "{'loss': 3.261, 'grad_norm': 9.224716186523438, 'learning_rate': 1.895e-05, 'epoch': 1.3}\n",
      "{'loss': 3.3026, 'grad_norm': 10.109212875366211, 'learning_rate': 1.8900000000000002e-05, 'epoch': 1.3}\n",
      "{'loss': 3.2076, 'grad_norm': 10.407818794250488, 'learning_rate': 1.885e-05, 'epoch': 1.3}\n",
      "{'loss': 3.2301, 'grad_norm': 9.829363822937012, 'learning_rate': 1.88e-05, 'epoch': 1.31}\n",
      "{'loss': 3.2516, 'grad_norm': 10.110617637634277, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.31}\n",
      "{'loss': 3.261, 'grad_norm': 9.57417106628418, 'learning_rate': 1.87e-05, 'epoch': 1.31}\n",
      "{'loss': 3.2572, 'grad_norm': 10.34600830078125, 'learning_rate': 1.865e-05, 'epoch': 1.31}\n",
      "{'loss': 3.2257, 'grad_norm': 9.84582233428955, 'learning_rate': 1.86e-05, 'epoch': 1.32}\n",
      "{'loss': 3.251, 'grad_norm': 10.482324600219727, 'learning_rate': 1.855e-05, 'epoch': 1.32}\n",
      "{'loss': 3.2938, 'grad_norm': 9.481008529663086, 'learning_rate': 1.85e-05, 'epoch': 1.32}\n",
      "{'loss': 3.271, 'grad_norm': 10.29596996307373, 'learning_rate': 1.845e-05, 'epoch': 1.32}\n",
      "{'loss': 3.2723, 'grad_norm': 10.427196502685547, 'learning_rate': 1.84e-05, 'epoch': 1.32}\n",
      "{'loss': 3.3014, 'grad_norm': 9.667728424072266, 'learning_rate': 1.8350000000000002e-05, 'epoch': 1.33}\n",
      "{'loss': 3.2374, 'grad_norm': 9.718706130981445, 'learning_rate': 1.83e-05, 'epoch': 1.33}\n",
      "{'loss': 3.2559, 'grad_norm': 10.54218578338623, 'learning_rate': 1.825e-05, 'epoch': 1.33}\n",
      "{'loss': 3.219, 'grad_norm': 10.280967712402344, 'learning_rate': 1.8200000000000002e-05, 'epoch': 1.33}\n",
      "{'loss': 3.3154, 'grad_norm': 9.877939224243164, 'learning_rate': 1.815e-05, 'epoch': 1.33}\n",
      "{'loss': 3.2479, 'grad_norm': 9.961506843566895, 'learning_rate': 1.81e-05, 'epoch': 1.34}\n",
      "{'loss': 3.2434, 'grad_norm': 9.555501937866211, 'learning_rate': 1.805e-05, 'epoch': 1.34}\n",
      "{'loss': 3.2278, 'grad_norm': 11.59091854095459, 'learning_rate': 1.8e-05, 'epoch': 1.34}\n",
      " 64%|█████████████████████            | 19200/30000 [4:38:43<2:25:08,  1.24it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.04s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.39s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.631719999999994, 'eval_rouge-2': 7.893365999999999, 'eval_rouge-l': 25.985681999999997, 'eval_bleu-4': 0.03956132305995502, 'eval_runtime': 30.464, 'eval_samples_per_second': 1.641, 'eval_steps_per_second': 0.131, 'epoch': 1.34}\n",
      " 64%|█████████████████████            | 19200/30000 [4:39:14<2:25:08,  1.24it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.47s/it]\u001b[A\n",
      "{'loss': 3.3352, 'grad_norm': 10.977483749389648, 'learning_rate': 1.795e-05, 'epoch': 1.34}\n",
      "{'loss': 3.2384, 'grad_norm': 10.891738891601562, 'learning_rate': 1.79e-05, 'epoch': 1.34}\n",
      "{'loss': 3.2305, 'grad_norm': 10.363105773925781, 'learning_rate': 1.785e-05, 'epoch': 1.35}\n",
      "{'loss': 3.255, 'grad_norm': 12.227107048034668, 'learning_rate': 1.78e-05, 'epoch': 1.35}\n",
      "{'loss': 3.2539, 'grad_norm': 11.125353813171387, 'learning_rate': 1.775e-05, 'epoch': 1.35}\n",
      "{'loss': 3.2648, 'grad_norm': 9.807172775268555, 'learning_rate': 1.77e-05, 'epoch': 1.35}\n",
      "{'loss': 3.2966, 'grad_norm': 9.317584037780762, 'learning_rate': 1.765e-05, 'epoch': 1.35}\n",
      "{'loss': 3.3301, 'grad_norm': 11.113234519958496, 'learning_rate': 1.76e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2676, 'grad_norm': 9.27495288848877, 'learning_rate': 1.755e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2945, 'grad_norm': 9.976841926574707, 'learning_rate': 1.75e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2294, 'grad_norm': 9.546263694763184, 'learning_rate': 1.745e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2253, 'grad_norm': 9.85659122467041, 'learning_rate': 1.74e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2906, 'grad_norm': 9.913326263427734, 'learning_rate': 1.7349999999999998e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2993, 'grad_norm': 10.226824760437012, 'learning_rate': 1.73e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2926, 'grad_norm': 9.822299003601074, 'learning_rate': 1.725e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2645, 'grad_norm': 11.611948013305664, 'learning_rate': 1.7199999999999998e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2482, 'grad_norm': 10.296295166015625, 'learning_rate': 1.7150000000000004e-05, 'epoch': 1.38}\n",
      "{'loss': 3.2178, 'grad_norm': 10.347062110900879, 'learning_rate': 1.7100000000000002e-05, 'epoch': 1.38}\n",
      "{'loss': 3.2674, 'grad_norm': 13.58138656616211, 'learning_rate': 1.705e-05, 'epoch': 1.38}\n",
      "{'loss': 3.2408, 'grad_norm': 10.710695266723633, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.38}\n",
      " 66%|█████████████████████▊           | 19800/30000 [4:47:19<2:11:06,  1.30it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.98s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.97s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.510362, 'eval_rouge-2': 7.970388000000002, 'eval_rouge-l': 25.451740000000004, 'eval_bleu-4': 0.039114282683574995, 'eval_runtime': 19.7145, 'eval_samples_per_second': 2.536, 'eval_steps_per_second': 0.203, 'epoch': 1.38}\n",
      " 66%|█████████████████████▊           | 19800/30000 [4:47:39<2:11:06,  1.30it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:13<00:00,  3.42s/it]\u001b[A\n",
      "{'loss': 3.2217, 'grad_norm': 10.884650230407715, 'learning_rate': 1.6950000000000002e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1828, 'grad_norm': 9.707077980041504, 'learning_rate': 1.69e-05, 'epoch': 1.39}\n",
      "{'loss': 3.2402, 'grad_norm': 9.845780372619629, 'learning_rate': 1.6850000000000003e-05, 'epoch': 1.39}\n",
      "{'loss': 3.2817, 'grad_norm': 10.194478034973145, 'learning_rate': 1.6800000000000002e-05, 'epoch': 1.39}\n",
      "{'loss': 3.2598, 'grad_norm': 9.940302848815918, 'learning_rate': 1.675e-05, 'epoch': 1.39}\n",
      "{'loss': 3.2309, 'grad_norm': 9.953886985778809, 'learning_rate': 1.6700000000000003e-05, 'epoch': 1.39}\n",
      "{'loss': 3.2115, 'grad_norm': 10.030573844909668, 'learning_rate': 1.665e-05, 'epoch': 1.4}\n",
      "{'loss': 3.231, 'grad_norm': 11.397772789001465, 'learning_rate': 1.66e-05, 'epoch': 1.4}\n",
      "{'loss': 3.2869, 'grad_norm': 10.54632568359375, 'learning_rate': 1.6550000000000002e-05, 'epoch': 1.4}\n",
      "{'loss': 3.2333, 'grad_norm': 9.808321952819824, 'learning_rate': 1.65e-05, 'epoch': 1.4}\n",
      "{'loss': 3.2786, 'grad_norm': 9.995827674865723, 'learning_rate': 1.645e-05, 'epoch': 1.41}\n",
      "{'loss': 3.2437, 'grad_norm': 10.6089448928833, 'learning_rate': 1.6400000000000002e-05, 'epoch': 1.41}\n",
      "{'loss': 3.2898, 'grad_norm': 9.594139099121094, 'learning_rate': 1.635e-05, 'epoch': 1.41}\n",
      "{'loss': 3.2389, 'grad_norm': 10.508955955505371, 'learning_rate': 1.63e-05, 'epoch': 1.41}\n",
      "{'loss': 3.2993, 'grad_norm': 10.500584602355957, 'learning_rate': 1.6250000000000002e-05, 'epoch': 1.41}\n",
      "{'loss': 3.2604, 'grad_norm': 10.283214569091797, 'learning_rate': 1.62e-05, 'epoch': 1.42}\n",
      "{'loss': 3.3202, 'grad_norm': 10.065874099731445, 'learning_rate': 1.6150000000000003e-05, 'epoch': 1.42}\n",
      "{'loss': 3.2325, 'grad_norm': 10.509344100952148, 'learning_rate': 1.6100000000000002e-05, 'epoch': 1.42}\n",
      "{'loss': 3.2089, 'grad_norm': 10.16839599609375, 'learning_rate': 1.605e-05, 'epoch': 1.42}\n",
      "{'loss': 3.2434, 'grad_norm': 12.41553020477295, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.42}\n",
      " 68%|██████████████████████▍          | 20400/30000 [4:55:43<2:16:29,  1.17it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.07s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.57s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.014998, 'eval_rouge-2': 8.049928000000001, 'eval_rouge-l': 25.863642000000006, 'eval_bleu-4': 0.037942840783892824, 'eval_runtime': 13.1195, 'eval_samples_per_second': 3.811, 'eval_steps_per_second': 0.305, 'epoch': 1.42}\n",
      " 68%|██████████████████████▍          | 20400/30000 [4:55:56<2:16:29,  1.17it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.60s/it]\u001b[A\n",
      "{'loss': 3.2888, 'grad_norm': 10.871395111083984, 'learning_rate': 1.595e-05, 'epoch': 1.43}\n",
      "{'loss': 3.3462, 'grad_norm': 9.910017013549805, 'learning_rate': 1.59e-05, 'epoch': 1.43}\n",
      "{'loss': 3.2145, 'grad_norm': 9.900507926940918, 'learning_rate': 1.5850000000000002e-05, 'epoch': 1.43}\n",
      "{'loss': 3.2493, 'grad_norm': 12.109088897705078, 'learning_rate': 1.58e-05, 'epoch': 1.43}\n",
      "{'loss': 3.2941, 'grad_norm': 9.949117660522461, 'learning_rate': 1.575e-05, 'epoch': 1.43}\n",
      "{'loss': 3.2779, 'grad_norm': 12.662196159362793, 'learning_rate': 1.5700000000000002e-05, 'epoch': 1.44}\n",
      "{'loss': 3.227, 'grad_norm': 10.83749008178711, 'learning_rate': 1.565e-05, 'epoch': 1.44}\n",
      "{'loss': 3.1389, 'grad_norm': 9.97137451171875, 'learning_rate': 1.56e-05, 'epoch': 1.44}\n",
      "{'loss': 3.2674, 'grad_norm': 9.57493782043457, 'learning_rate': 1.5550000000000002e-05, 'epoch': 1.44}\n",
      "{'loss': 3.2789, 'grad_norm': 10.123271942138672, 'learning_rate': 1.55e-05, 'epoch': 1.45}\n",
      "{'loss': 3.2714, 'grad_norm': 10.170639991760254, 'learning_rate': 1.545e-05, 'epoch': 1.45}\n",
      "{'loss': 3.2098, 'grad_norm': 10.280535697937012, 'learning_rate': 1.54e-05, 'epoch': 1.45}\n",
      "{'loss': 3.3075, 'grad_norm': 9.873997688293457, 'learning_rate': 1.535e-05, 'epoch': 1.45}\n",
      "{'loss': 3.2828, 'grad_norm': 9.931180953979492, 'learning_rate': 1.53e-05, 'epoch': 1.45}\n",
      "{'loss': 3.2943, 'grad_norm': 10.193498611450195, 'learning_rate': 1.525e-05, 'epoch': 1.46}\n",
      "{'loss': 3.2231, 'grad_norm': 10.274553298950195, 'learning_rate': 1.52e-05, 'epoch': 1.46}\n",
      "{'loss': 3.3105, 'grad_norm': 9.607105255126953, 'learning_rate': 1.515e-05, 'epoch': 1.46}\n",
      "{'loss': 3.31, 'grad_norm': 11.865859031677246, 'learning_rate': 1.51e-05, 'epoch': 1.46}\n",
      "{'loss': 3.28, 'grad_norm': 11.617916107177734, 'learning_rate': 1.505e-05, 'epoch': 1.46}\n",
      "{'loss': 3.2425, 'grad_norm': 9.923393249511719, 'learning_rate': 1.5e-05, 'epoch': 1.47}\n",
      " 70%|███████████████████████          | 21000/30000 [5:03:59<2:14:27,  1.12it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.35s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:03,  3.04s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.93285, 'eval_rouge-2': 7.796066, 'eval_rouge-l': 25.44367, 'eval_bleu-4': 0.0388039983394505, 'eval_runtime': 15.4582, 'eval_samples_per_second': 3.235, 'eval_steps_per_second': 0.259, 'epoch': 1.47}\n",
      " 70%|███████████████████████          | 21000/30000 [5:04:14<2:14:27,  1.12it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:11<00:00,  2.96s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-21000\n",
      "tokenizer config file saved in ./output/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-21000/special_tokens_map.json\n",
      "{'loss': 3.2665, 'grad_norm': 11.140883445739746, 'learning_rate': 1.4950000000000001e-05, 'epoch': 1.47}\n",
      "{'loss': 3.2447, 'grad_norm': 10.906390190124512, 'learning_rate': 1.49e-05, 'epoch': 1.47}\n",
      "{'loss': 3.2611, 'grad_norm': 10.035765647888184, 'learning_rate': 1.485e-05, 'epoch': 1.47}\n",
      "{'loss': 3.3111, 'grad_norm': 9.90186595916748, 'learning_rate': 1.48e-05, 'epoch': 1.47}\n",
      "{'loss': 3.2267, 'grad_norm': 10.136570930480957, 'learning_rate': 1.475e-05, 'epoch': 1.48}\n",
      "{'loss': 3.2827, 'grad_norm': 10.286602973937988, 'learning_rate': 1.47e-05, 'epoch': 1.48}\n",
      "{'loss': 3.2563, 'grad_norm': 9.711202621459961, 'learning_rate': 1.465e-05, 'epoch': 1.48}\n",
      "{'loss': 3.286, 'grad_norm': 10.676258087158203, 'learning_rate': 1.4599999999999999e-05, 'epoch': 1.48}\n",
      "{'loss': 3.1619, 'grad_norm': 10.494368553161621, 'learning_rate': 1.455e-05, 'epoch': 1.48}\n",
      "{'loss': 3.2045, 'grad_norm': 10.91607666015625, 'learning_rate': 1.45e-05, 'epoch': 1.49}\n",
      "{'loss': 3.2317, 'grad_norm': 9.827173233032227, 'learning_rate': 1.4449999999999999e-05, 'epoch': 1.49}\n",
      "{'loss': 3.238, 'grad_norm': 10.002874374389648, 'learning_rate': 1.44e-05, 'epoch': 1.49}\n",
      "{'loss': 3.2501, 'grad_norm': 11.012884140014648, 'learning_rate': 1.435e-05, 'epoch': 1.49}\n",
      "{'loss': 3.2546, 'grad_norm': 9.700114250183105, 'learning_rate': 1.43e-05, 'epoch': 1.5}\n",
      "{'loss': 3.2652, 'grad_norm': 10.673619270324707, 'learning_rate': 1.4249999999999999e-05, 'epoch': 1.5}\n",
      "{'loss': 3.2767, 'grad_norm': 11.333985328674316, 'learning_rate': 1.42e-05, 'epoch': 1.5}\n",
      "{'loss': 3.2557, 'grad_norm': 12.020795822143555, 'learning_rate': 1.415e-05, 'epoch': 1.5}\n",
      "{'loss': 3.2343, 'grad_norm': 11.98701286315918, 'learning_rate': 1.4099999999999999e-05, 'epoch': 1.5}\n",
      "{'loss': 3.2398, 'grad_norm': 10.866168975830078, 'learning_rate': 1.4050000000000003e-05, 'epoch': 1.51}\n",
      "{'loss': 3.2751, 'grad_norm': 9.734926223754883, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.51}\n",
      " 72%|███████████████████████▊         | 21600/30000 [5:12:13<1:48:57,  1.28it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.41s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:09<00:03,  3.33s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.708754, 'eval_rouge-2': 8.66307, 'eval_rouge-l': 26.331117999999996, 'eval_bleu-4': 0.04214625862621418, 'eval_runtime': 15.6768, 'eval_samples_per_second': 3.189, 'eval_steps_per_second': 0.255, 'epoch': 1.51}\n",
      " 72%|███████████████████████▊         | 21600/30000 [5:12:28<1:48:57,  1.28it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:12<00:00,  3.14s/it]\u001b[A\n",
      "{'loss': 3.2714, 'grad_norm': 9.96630859375, 'learning_rate': 1.3950000000000002e-05, 'epoch': 1.51}\n",
      "{'loss': 3.2585, 'grad_norm': 10.752718925476074, 'learning_rate': 1.3900000000000002e-05, 'epoch': 1.51}\n",
      "{'loss': 3.3307, 'grad_norm': 10.70718765258789, 'learning_rate': 1.3850000000000001e-05, 'epoch': 1.51}\n",
      "{'loss': 3.2831, 'grad_norm': 10.015168190002441, 'learning_rate': 1.3800000000000002e-05, 'epoch': 1.52}\n",
      "{'loss': 3.2519, 'grad_norm': 9.995959281921387, 'learning_rate': 1.3750000000000002e-05, 'epoch': 1.52}\n",
      "{'loss': 3.2139, 'grad_norm': 9.771519660949707, 'learning_rate': 1.3700000000000001e-05, 'epoch': 1.52}\n",
      "{'loss': 3.2868, 'grad_norm': 11.648195266723633, 'learning_rate': 1.3650000000000001e-05, 'epoch': 1.52}\n",
      "{'loss': 3.2924, 'grad_norm': 10.21550464630127, 'learning_rate': 1.3600000000000002e-05, 'epoch': 1.52}\n",
      "{'loss': 3.2333, 'grad_norm': 11.569180488586426, 'learning_rate': 1.3550000000000002e-05, 'epoch': 1.53}\n",
      "{'loss': 3.2406, 'grad_norm': 10.637969970703125, 'learning_rate': 1.3500000000000001e-05, 'epoch': 1.53}\n",
      "{'loss': 3.2701, 'grad_norm': 9.990084648132324, 'learning_rate': 1.3450000000000002e-05, 'epoch': 1.53}\n",
      "{'loss': 3.2403, 'grad_norm': 9.741800308227539, 'learning_rate': 1.3400000000000002e-05, 'epoch': 1.53}\n",
      "{'loss': 3.1908, 'grad_norm': 11.458860397338867, 'learning_rate': 1.3350000000000001e-05, 'epoch': 1.54}\n",
      "{'loss': 3.293, 'grad_norm': 9.496957778930664, 'learning_rate': 1.3300000000000001e-05, 'epoch': 1.54}\n",
      "{'loss': 3.2663, 'grad_norm': 10.382369995117188, 'learning_rate': 1.3250000000000002e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1962, 'grad_norm': 11.263468742370605, 'learning_rate': 1.32e-05, 'epoch': 1.54}\n",
      "{'loss': 3.2985, 'grad_norm': 11.996582984924316, 'learning_rate': 1.3150000000000001e-05, 'epoch': 1.54}\n",
      "{'loss': 3.2992, 'grad_norm': 11.55972957611084, 'learning_rate': 1.3100000000000002e-05, 'epoch': 1.55}\n",
      "{'loss': 3.2427, 'grad_norm': 10.023612976074219, 'learning_rate': 1.305e-05, 'epoch': 1.55}\n",
      "{'loss': 3.2322, 'grad_norm': 11.181671142578125, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.55}\n",
      " 74%|████████████████████████▍        | 22200/30000 [5:20:35<1:48:40,  1.20it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.85s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.47s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.302544000000005, 'eval_rouge-2': 7.394995999999999, 'eval_rouge-l': 25.925182, 'eval_bleu-4': 0.03644056010094986, 'eval_runtime': 16.1155, 'eval_samples_per_second': 3.103, 'eval_steps_per_second': 0.248, 'epoch': 1.55}\n",
      " 74%|████████████████████████▍        | 22200/30000 [5:20:51<1:48:40,  1.20it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.52s/it]\u001b[A\n",
      "{'loss': 3.2442, 'grad_norm': 10.038365364074707, 'learning_rate': 1.2950000000000001e-05, 'epoch': 1.55}\n",
      "{'loss': 3.2843, 'grad_norm': 10.161524772644043, 'learning_rate': 1.29e-05, 'epoch': 1.55}\n",
      "{'loss': 3.2805, 'grad_norm': 10.41282844543457, 'learning_rate': 1.285e-05, 'epoch': 1.56}\n",
      "{'loss': 3.2286, 'grad_norm': 10.84388256072998, 'learning_rate': 1.2800000000000001e-05, 'epoch': 1.56}\n",
      "{'loss': 3.3084, 'grad_norm': 9.364813804626465, 'learning_rate': 1.2750000000000002e-05, 'epoch': 1.56}\n",
      "{'loss': 3.2049, 'grad_norm': 10.13221549987793, 'learning_rate': 1.27e-05, 'epoch': 1.56}\n",
      "{'loss': 3.1907, 'grad_norm': 10.807723045349121, 'learning_rate': 1.2650000000000001e-05, 'epoch': 1.56}\n",
      "{'loss': 3.22, 'grad_norm': 11.512279510498047, 'learning_rate': 1.2600000000000001e-05, 'epoch': 1.57}\n",
      "{'loss': 3.2313, 'grad_norm': 10.260198593139648, 'learning_rate': 1.255e-05, 'epoch': 1.57}\n",
      "{'loss': 3.1969, 'grad_norm': 10.60180950164795, 'learning_rate': 1.25e-05, 'epoch': 1.57}\n",
      "{'loss': 3.2702, 'grad_norm': 10.987159729003906, 'learning_rate': 1.2450000000000001e-05, 'epoch': 1.57}\n",
      "{'loss': 3.1771, 'grad_norm': 11.218061447143555, 'learning_rate': 1.24e-05, 'epoch': 1.57}\n",
      "{'loss': 3.2357, 'grad_norm': 11.848315238952637, 'learning_rate': 1.235e-05, 'epoch': 1.58}\n",
      "{'loss': 3.2617, 'grad_norm': 10.967233657836914, 'learning_rate': 1.23e-05, 'epoch': 1.58}\n",
      "{'loss': 3.2303, 'grad_norm': 10.746305465698242, 'learning_rate': 1.225e-05, 'epoch': 1.58}\n",
      "{'loss': 3.2554, 'grad_norm': 10.347176551818848, 'learning_rate': 1.22e-05, 'epoch': 1.58}\n",
      "{'loss': 3.2991, 'grad_norm': 10.641868591308594, 'learning_rate': 1.215e-05, 'epoch': 1.59}\n",
      "{'loss': 3.202, 'grad_norm': 9.487957954406738, 'learning_rate': 1.2100000000000001e-05, 'epoch': 1.59}\n",
      "{'loss': 3.2432, 'grad_norm': 10.804949760437012, 'learning_rate': 1.205e-05, 'epoch': 1.59}\n",
      "{'loss': 3.2018, 'grad_norm': 9.774613380432129, 'learning_rate': 1.2e-05, 'epoch': 1.59}\n",
      " 76%|█████████████████████████        | 22800/30000 [5:28:57<1:42:04,  1.18it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.93s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.52s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.929652000000004, 'eval_rouge-2': 7.9500839999999995, 'eval_rouge-l': 25.545816000000006, 'eval_bleu-4': 0.03772691056885622, 'eval_runtime': 13.6579, 'eval_samples_per_second': 3.661, 'eval_steps_per_second': 0.293, 'epoch': 1.59}\n",
      " 76%|█████████████████████████        | 22800/30000 [5:29:11<1:42:04,  1.18it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.52s/it]\u001b[A\n",
      "{'loss': 3.2474, 'grad_norm': 10.818697929382324, 'learning_rate': 1.195e-05, 'epoch': 1.59}\n",
      "{'loss': 3.2405, 'grad_norm': 10.992650032043457, 'learning_rate': 1.19e-05, 'epoch': 1.6}\n",
      "{'loss': 3.2464, 'grad_norm': 10.057003021240234, 'learning_rate': 1.185e-05, 'epoch': 1.6}\n",
      "{'loss': 3.2215, 'grad_norm': 10.86686897277832, 'learning_rate': 1.18e-05, 'epoch': 1.6}\n",
      "{'loss': 3.3321, 'grad_norm': 10.862787246704102, 'learning_rate': 1.175e-05, 'epoch': 1.6}\n",
      "{'loss': 3.2583, 'grad_norm': 10.485941886901855, 'learning_rate': 1.1700000000000001e-05, 'epoch': 1.6}\n",
      "{'loss': 3.2557, 'grad_norm': 10.363184928894043, 'learning_rate': 1.1650000000000002e-05, 'epoch': 1.61}\n",
      "{'loss': 3.2724, 'grad_norm': 10.24812126159668, 'learning_rate': 1.16e-05, 'epoch': 1.61}\n",
      "{'loss': 3.2133, 'grad_norm': 12.166916847229004, 'learning_rate': 1.1550000000000001e-05, 'epoch': 1.61}\n",
      "{'loss': 3.2365, 'grad_norm': 10.398513793945312, 'learning_rate': 1.1500000000000002e-05, 'epoch': 1.61}\n",
      "{'loss': 3.2319, 'grad_norm': 11.783878326416016, 'learning_rate': 1.145e-05, 'epoch': 1.61}\n",
      "{'loss': 3.2342, 'grad_norm': 10.392351150512695, 'learning_rate': 1.1400000000000001e-05, 'epoch': 1.62}\n",
      "{'loss': 3.2898, 'grad_norm': 10.744157791137695, 'learning_rate': 1.1350000000000001e-05, 'epoch': 1.62}\n",
      "{'loss': 3.3052, 'grad_norm': 11.686065673828125, 'learning_rate': 1.13e-05, 'epoch': 1.62}\n",
      "{'loss': 3.2613, 'grad_norm': 10.160558700561523, 'learning_rate': 1.125e-05, 'epoch': 1.62}\n",
      "{'loss': 3.2422, 'grad_norm': 10.626596450805664, 'learning_rate': 1.1200000000000001e-05, 'epoch': 1.63}\n",
      "{'loss': 3.2767, 'grad_norm': 10.176212310791016, 'learning_rate': 1.115e-05, 'epoch': 1.63}\n",
      "{'loss': 3.2579, 'grad_norm': 11.567198753356934, 'learning_rate': 1.11e-05, 'epoch': 1.63}\n",
      "{'loss': 3.2285, 'grad_norm': 10.802103042602539, 'learning_rate': 1.1050000000000001e-05, 'epoch': 1.63}\n",
      "{'loss': 3.2729, 'grad_norm': 11.032493591308594, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.63}\n",
      " 78%|█████████████████████████▋       | 23400/30000 [5:37:17<1:28:34,  1.24it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.05s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.40s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.669478, 'eval_rouge-2': 7.420882000000001, 'eval_rouge-l': 25.191508, 'eval_bleu-4': 0.03399379656804187, 'eval_runtime': 14.8749, 'eval_samples_per_second': 3.361, 'eval_steps_per_second': 0.269, 'epoch': 1.63}\n",
      " 78%|█████████████████████████▋       | 23400/30000 [5:37:31<1:28:34,  1.24it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.62s/it]\u001b[A\n",
      "{'loss': 3.2181, 'grad_norm': 11.010693550109863, 'learning_rate': 1.095e-05, 'epoch': 1.64}\n",
      "{'loss': 3.2176, 'grad_norm': 11.480319023132324, 'learning_rate': 1.09e-05, 'epoch': 1.64}\n",
      "{'loss': 3.184, 'grad_norm': 10.066987037658691, 'learning_rate': 1.0850000000000001e-05, 'epoch': 1.64}\n",
      "{'loss': 3.2364, 'grad_norm': 10.65195369720459, 'learning_rate': 1.08e-05, 'epoch': 1.64}\n",
      "{'loss': 3.2463, 'grad_norm': 11.827659606933594, 'learning_rate': 1.075e-05, 'epoch': 1.64}\n",
      "{'loss': 3.2257, 'grad_norm': 9.86599063873291, 'learning_rate': 1.0700000000000001e-05, 'epoch': 1.65}\n",
      "{'loss': 3.2502, 'grad_norm': 10.697017669677734, 'learning_rate': 1.065e-05, 'epoch': 1.65}\n",
      "{'loss': 3.2623, 'grad_norm': 11.918082237243652, 'learning_rate': 1.06e-05, 'epoch': 1.65}\n",
      "{'loss': 3.268, 'grad_norm': 11.080004692077637, 'learning_rate': 1.055e-05, 'epoch': 1.65}\n",
      "{'loss': 3.2377, 'grad_norm': 10.609752655029297, 'learning_rate': 1.05e-05, 'epoch': 1.65}\n",
      "{'loss': 3.286, 'grad_norm': 11.731669425964355, 'learning_rate': 1.045e-05, 'epoch': 1.66}\n",
      "{'loss': 3.238, 'grad_norm': 10.319574356079102, 'learning_rate': 1.04e-05, 'epoch': 1.66}\n",
      "{'loss': 3.2799, 'grad_norm': 11.694520950317383, 'learning_rate': 1.035e-05, 'epoch': 1.66}\n",
      "{'loss': 3.3083, 'grad_norm': 11.233255386352539, 'learning_rate': 1.03e-05, 'epoch': 1.66}\n",
      "{'loss': 3.2971, 'grad_norm': 10.816450119018555, 'learning_rate': 1.025e-05, 'epoch': 1.66}\n",
      "{'loss': 3.2685, 'grad_norm': 11.160443305969238, 'learning_rate': 1.02e-05, 'epoch': 1.67}\n",
      "{'loss': 3.2545, 'grad_norm': 11.890777587890625, 'learning_rate': 1.0150000000000001e-05, 'epoch': 1.67}\n",
      "{'loss': 3.2046, 'grad_norm': 9.66723918914795, 'learning_rate': 1.0100000000000002e-05, 'epoch': 1.67}\n",
      "{'loss': 3.2257, 'grad_norm': 10.706459999084473, 'learning_rate': 1.005e-05, 'epoch': 1.67}\n",
      "{'loss': 3.2921, 'grad_norm': 10.657991409301758, 'learning_rate': 1e-05, 'epoch': 1.68}\n",
      " 80%|██████████████████████████▍      | 24000/30000 [5:45:35<1:24:00,  1.19it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.75s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:02,  2.88s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.585772, 'eval_rouge-2': 7.755321999999999, 'eval_rouge-l': 25.875778000000004, 'eval_bleu-4': 0.03670122382418509, 'eval_runtime': 16.6789, 'eval_samples_per_second': 2.998, 'eval_steps_per_second': 0.24, 'epoch': 1.68}\n",
      " 80%|██████████████████████████▍      | 24000/30000 [5:45:52<1:24:00,  1.19it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:11<00:00,  2.72s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-24000\n",
      "tokenizer config file saved in ./output/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-24000/special_tokens_map.json\n",
      "{'loss': 3.2629, 'grad_norm': 12.262643814086914, 'learning_rate': 9.950000000000001e-06, 'epoch': 1.68}\n",
      "{'loss': 3.2288, 'grad_norm': 10.619620323181152, 'learning_rate': 9.900000000000002e-06, 'epoch': 1.68}\n",
      "{'loss': 3.235, 'grad_norm': 11.674856185913086, 'learning_rate': 9.85e-06, 'epoch': 1.68}\n",
      "{'loss': 3.2135, 'grad_norm': 9.980583190917969, 'learning_rate': 9.800000000000001e-06, 'epoch': 1.68}\n",
      "{'loss': 3.2432, 'grad_norm': 11.964035987854004, 'learning_rate': 9.750000000000002e-06, 'epoch': 1.69}\n",
      "{'loss': 3.27, 'grad_norm': 11.555673599243164, 'learning_rate': 9.7e-06, 'epoch': 1.69}\n",
      "{'loss': 3.315, 'grad_norm': 11.591938972473145, 'learning_rate': 9.65e-06, 'epoch': 1.69}\n",
      "{'loss': 3.262, 'grad_norm': 11.072251319885254, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.69}\n",
      "{'loss': 3.2775, 'grad_norm': 10.71728515625, 'learning_rate': 9.55e-06, 'epoch': 1.69}\n",
      "{'loss': 3.2208, 'grad_norm': 11.544270515441895, 'learning_rate': 9.5e-06, 'epoch': 1.7}\n",
      "{'loss': 3.2657, 'grad_norm': 12.667421340942383, 'learning_rate': 9.450000000000001e-06, 'epoch': 1.7}\n",
      "{'loss': 3.2326, 'grad_norm': 9.86317253112793, 'learning_rate': 9.4e-06, 'epoch': 1.7}\n",
      "{'loss': 3.2119, 'grad_norm': 12.968803405761719, 'learning_rate': 9.35e-06, 'epoch': 1.7}\n",
      "{'loss': 3.2043, 'grad_norm': 12.563626289367676, 'learning_rate': 9.3e-06, 'epoch': 1.7}\n",
      "{'loss': 3.2632, 'grad_norm': 11.192066192626953, 'learning_rate': 9.25e-06, 'epoch': 1.71}\n",
      "{'loss': 3.2561, 'grad_norm': 11.853338241577148, 'learning_rate': 9.2e-06, 'epoch': 1.71}\n",
      "{'loss': 3.2872, 'grad_norm': 10.14185905456543, 'learning_rate': 9.15e-06, 'epoch': 1.71}\n",
      "{'loss': 3.2552, 'grad_norm': 10.089489936828613, 'learning_rate': 9.100000000000001e-06, 'epoch': 1.71}\n",
      "{'loss': 3.249, 'grad_norm': 10.999771118164062, 'learning_rate': 9.05e-06, 'epoch': 1.72}\n",
      "{'loss': 3.2199, 'grad_norm': 11.369978904724121, 'learning_rate': 9e-06, 'epoch': 1.72}\n",
      " 82%|███████████████████████████      | 24600/30000 [5:53:56<1:14:22,  1.21it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.61s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:08<00:03,  3.06s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.406502, 'eval_rouge-2': 7.164254, 'eval_rouge-l': 25.036594, 'eval_bleu-4': 0.03394763306269199, 'eval_runtime': 17.8319, 'eval_samples_per_second': 2.804, 'eval_steps_per_second': 0.224, 'epoch': 1.72}\n",
      " 82%|███████████████████████████      | 24600/30000 [5:54:14<1:14:22,  1.21it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:11<00:00,  2.82s/it]\u001b[A\n",
      "{'loss': 3.285, 'grad_norm': 10.91132926940918, 'learning_rate': 8.95e-06, 'epoch': 1.72}\n",
      "{'loss': 3.2491, 'grad_norm': 10.909802436828613, 'learning_rate': 8.9e-06, 'epoch': 1.72}\n",
      "{'loss': 3.2745, 'grad_norm': 11.369730949401855, 'learning_rate': 8.85e-06, 'epoch': 1.72}\n",
      "{'loss': 3.1744, 'grad_norm': 9.902139663696289, 'learning_rate': 8.8e-06, 'epoch': 1.73}\n",
      "{'loss': 3.259, 'grad_norm': 11.001211166381836, 'learning_rate': 8.75e-06, 'epoch': 1.73}\n",
      "{'loss': 3.2261, 'grad_norm': 11.448542594909668, 'learning_rate': 8.7e-06, 'epoch': 1.73}\n",
      "{'loss': 3.2383, 'grad_norm': 10.200444221496582, 'learning_rate': 8.65e-06, 'epoch': 1.73}\n",
      "{'loss': 3.31, 'grad_norm': 11.33619213104248, 'learning_rate': 8.599999999999999e-06, 'epoch': 1.73}\n",
      "{'loss': 3.1917, 'grad_norm': 11.207257270812988, 'learning_rate': 8.550000000000001e-06, 'epoch': 1.74}\n",
      "{'loss': 3.2163, 'grad_norm': 11.454325675964355, 'learning_rate': 8.500000000000002e-06, 'epoch': 1.74}\n",
      "{'loss': 3.2564, 'grad_norm': 10.209774017333984, 'learning_rate': 8.45e-06, 'epoch': 1.74}\n",
      "{'loss': 3.2483, 'grad_norm': 10.356907844543457, 'learning_rate': 8.400000000000001e-06, 'epoch': 1.74}\n",
      "{'loss': 3.2858, 'grad_norm': 9.857763290405273, 'learning_rate': 8.350000000000001e-06, 'epoch': 1.74}\n",
      "{'loss': 3.2599, 'grad_norm': 10.344170570373535, 'learning_rate': 8.3e-06, 'epoch': 1.75}\n",
      "{'loss': 3.2333, 'grad_norm': 11.2296781539917, 'learning_rate': 8.25e-06, 'epoch': 1.75}\n",
      "{'loss': 3.2714, 'grad_norm': 10.781898498535156, 'learning_rate': 8.200000000000001e-06, 'epoch': 1.75}\n",
      "{'loss': 3.2723, 'grad_norm': 11.639771461486816, 'learning_rate': 8.15e-06, 'epoch': 1.75}\n",
      "{'loss': 3.2912, 'grad_norm': 9.647290229797363, 'learning_rate': 8.1e-06, 'epoch': 1.75}\n",
      "{'loss': 3.241, 'grad_norm': 10.750731468200684, 'learning_rate': 8.050000000000001e-06, 'epoch': 1.76}\n",
      "{'loss': 3.205, 'grad_norm': 11.481239318847656, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.76}\n",
      " 84%|███████████████████████████▋     | 25200/30000 [6:02:16<1:02:25,  1.28it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.95s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.37s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.581314, 'eval_rouge-2': 8.59104, 'eval_rouge-l': 26.531246, 'eval_bleu-4': 0.03930849369608043, 'eval_runtime': 13.2762, 'eval_samples_per_second': 3.766, 'eval_steps_per_second': 0.301, 'epoch': 1.76}\n",
      " 84%|███████████████████████████▋     | 25200/30000 [6:02:30<1:02:25,  1.28it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.47s/it]\u001b[A\n",
      "{'loss': 3.2276, 'grad_norm': 10.693452835083008, 'learning_rate': 7.95e-06, 'epoch': 1.76}\n",
      "{'loss': 3.2237, 'grad_norm': 10.234786987304688, 'learning_rate': 7.9e-06, 'epoch': 1.76}\n",
      "{'loss': 3.2618, 'grad_norm': 10.630509376525879, 'learning_rate': 7.850000000000001e-06, 'epoch': 1.77}\n",
      "{'loss': 3.2419, 'grad_norm': 10.25482177734375, 'learning_rate': 7.8e-06, 'epoch': 1.77}\n",
      "{'loss': 3.3085, 'grad_norm': 10.7053804397583, 'learning_rate': 7.75e-06, 'epoch': 1.77}\n",
      "{'loss': 3.2737, 'grad_norm': 10.1436128616333, 'learning_rate': 7.7e-06, 'epoch': 1.77}\n",
      "{'loss': 3.2559, 'grad_norm': 10.056199073791504, 'learning_rate': 7.65e-06, 'epoch': 1.77}\n",
      "{'loss': 3.268, 'grad_norm': 11.596441268920898, 'learning_rate': 7.6e-06, 'epoch': 1.78}\n",
      "{'loss': 3.2193, 'grad_norm': 11.242331504821777, 'learning_rate': 7.55e-06, 'epoch': 1.78}\n",
      "{'loss': 3.2795, 'grad_norm': 12.221927642822266, 'learning_rate': 7.5e-06, 'epoch': 1.78}\n",
      "{'loss': 3.2083, 'grad_norm': 11.614890098571777, 'learning_rate': 7.45e-06, 'epoch': 1.78}\n",
      "{'loss': 3.2551, 'grad_norm': 11.352773666381836, 'learning_rate': 7.4e-06, 'epoch': 1.78}\n",
      "{'loss': 3.2272, 'grad_norm': 11.21053695678711, 'learning_rate': 7.35e-06, 'epoch': 1.79}\n",
      "{'loss': 3.227, 'grad_norm': 10.832393646240234, 'learning_rate': 7.2999999999999996e-06, 'epoch': 1.79}\n",
      "{'loss': 3.2014, 'grad_norm': 10.831550598144531, 'learning_rate': 7.25e-06, 'epoch': 1.79}\n",
      "{'loss': 3.2093, 'grad_norm': 11.56704044342041, 'learning_rate': 7.2e-06, 'epoch': 1.79}\n",
      "{'loss': 3.1866, 'grad_norm': 10.693537712097168, 'learning_rate': 7.15e-06, 'epoch': 1.79}\n",
      "{'loss': 3.2488, 'grad_norm': 11.043095588684082, 'learning_rate': 7.1e-06, 'epoch': 1.8}\n",
      "{'loss': 3.224, 'grad_norm': 10.451057434082031, 'learning_rate': 7.049999999999999e-06, 'epoch': 1.8}\n",
      "{'loss': 3.2176, 'grad_norm': 10.53844165802002, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.8}\n",
      " 86%|██████████████████████████████     | 25800/30000 [6:10:32<59:18,  1.18it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.36s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.47s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.26376, 'eval_rouge-2': 8.431022, 'eval_rouge-l': 26.257437999999997, 'eval_bleu-4': 0.0381287933333353, 'eval_runtime': 13.2739, 'eval_samples_per_second': 3.767, 'eval_steps_per_second': 0.301, 'epoch': 1.8}\n",
      " 86%|██████████████████████████████     | 25800/30000 [6:10:45<59:18,  1.18it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.30s/it]\u001b[A\n",
      "{'loss': 3.2526, 'grad_norm': 10.760676383972168, 'learning_rate': 6.950000000000001e-06, 'epoch': 1.8}\n",
      "{'loss': 3.2773, 'grad_norm': 10.442727088928223, 'learning_rate': 6.900000000000001e-06, 'epoch': 1.81}\n",
      "{'loss': 3.2342, 'grad_norm': 11.251620292663574, 'learning_rate': 6.8500000000000005e-06, 'epoch': 1.81}\n",
      "{'loss': 3.2392, 'grad_norm': 11.146618843078613, 'learning_rate': 6.800000000000001e-06, 'epoch': 1.81}\n",
      "{'loss': 3.2027, 'grad_norm': 11.157123565673828, 'learning_rate': 6.750000000000001e-06, 'epoch': 1.81}\n",
      "{'loss': 3.2559, 'grad_norm': 11.331469535827637, 'learning_rate': 6.700000000000001e-06, 'epoch': 1.81}\n",
      "{'loss': 3.2717, 'grad_norm': 12.028635025024414, 'learning_rate': 6.650000000000001e-06, 'epoch': 1.82}\n",
      "{'loss': 3.2497, 'grad_norm': 13.108827590942383, 'learning_rate': 6.6e-06, 'epoch': 1.82}\n",
      "{'loss': 3.2644, 'grad_norm': 11.4874267578125, 'learning_rate': 6.550000000000001e-06, 'epoch': 1.82}\n",
      "{'loss': 3.2747, 'grad_norm': 10.932147026062012, 'learning_rate': 6.5000000000000004e-06, 'epoch': 1.82}\n",
      "{'loss': 3.2569, 'grad_norm': 12.487366676330566, 'learning_rate': 6.45e-06, 'epoch': 1.82}\n",
      "{'loss': 3.2337, 'grad_norm': 12.156335830688477, 'learning_rate': 6.4000000000000006e-06, 'epoch': 1.83}\n",
      "{'loss': 3.26, 'grad_norm': 9.886404991149902, 'learning_rate': 6.35e-06, 'epoch': 1.83}\n",
      "{'loss': 3.2051, 'grad_norm': 11.093361854553223, 'learning_rate': 6.300000000000001e-06, 'epoch': 1.83}\n",
      "{'loss': 3.301, 'grad_norm': 10.683438301086426, 'learning_rate': 6.25e-06, 'epoch': 1.83}\n",
      "{'loss': 3.2386, 'grad_norm': 11.220661163330078, 'learning_rate': 6.2e-06, 'epoch': 1.83}\n",
      "{'loss': 3.2648, 'grad_norm': 10.771541595458984, 'learning_rate': 6.15e-06, 'epoch': 1.84}\n",
      "{'loss': 3.2568, 'grad_norm': 11.376708030700684, 'learning_rate': 6.1e-06, 'epoch': 1.84}\n",
      "{'loss': 3.2905, 'grad_norm': 12.26447582244873, 'learning_rate': 6.0500000000000005e-06, 'epoch': 1.84}\n",
      "{'loss': 3.206, 'grad_norm': 11.012782096862793, 'learning_rate': 6e-06, 'epoch': 1.84}\n",
      " 88%|██████████████████████████████▊    | 26400/30000 [6:18:46<47:27,  1.26it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.41s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:09<00:03,  3.35s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.43741, 'eval_rouge-2': 7.780405999999999, 'eval_rouge-l': 25.850471999999996, 'eval_bleu-4': 0.037453368235770466, 'eval_runtime': 19.2321, 'eval_samples_per_second': 2.6, 'eval_steps_per_second': 0.208, 'epoch': 1.84}\n",
      " 88%|██████████████████████████████▊    | 26400/30000 [6:19:05<47:27,  1.26it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  3.97s/it]\u001b[A\n",
      "{'loss': 3.2807, 'grad_norm': 11.302791595458984, 'learning_rate': 5.95e-06, 'epoch': 1.85}\n",
      "{'loss': 3.2702, 'grad_norm': 12.294377326965332, 'learning_rate': 5.9e-06, 'epoch': 1.85}\n",
      "{'loss': 3.2359, 'grad_norm': 11.83345890045166, 'learning_rate': 5.850000000000001e-06, 'epoch': 1.85}\n",
      "{'loss': 3.1942, 'grad_norm': 11.082438468933105, 'learning_rate': 5.8e-06, 'epoch': 1.85}\n",
      "{'loss': 3.2441, 'grad_norm': 11.820956230163574, 'learning_rate': 5.750000000000001e-06, 'epoch': 1.85}\n",
      "{'loss': 3.2031, 'grad_norm': 11.417020797729492, 'learning_rate': 5.7000000000000005e-06, 'epoch': 1.86}\n",
      "{'loss': 3.2259, 'grad_norm': 12.661992073059082, 'learning_rate': 5.65e-06, 'epoch': 1.86}\n",
      "{'loss': 3.2315, 'grad_norm': 11.889385223388672, 'learning_rate': 5.600000000000001e-06, 'epoch': 1.86}\n",
      "{'loss': 3.2075, 'grad_norm': 11.441083908081055, 'learning_rate': 5.55e-06, 'epoch': 1.86}\n",
      "{'loss': 3.3055, 'grad_norm': 11.236701011657715, 'learning_rate': 5.500000000000001e-06, 'epoch': 1.86}\n",
      "{'loss': 3.2749, 'grad_norm': 11.430359840393066, 'learning_rate': 5.45e-06, 'epoch': 1.87}\n",
      "{'loss': 3.2456, 'grad_norm': 11.778024673461914, 'learning_rate': 5.4e-06, 'epoch': 1.87}\n",
      "{'loss': 3.2441, 'grad_norm': 12.28207015991211, 'learning_rate': 5.3500000000000004e-06, 'epoch': 1.87}\n",
      "{'loss': 3.2906, 'grad_norm': 11.812735557556152, 'learning_rate': 5.3e-06, 'epoch': 1.87}\n",
      "{'loss': 3.2041, 'grad_norm': 16.59423065185547, 'learning_rate': 5.25e-06, 'epoch': 1.87}\n",
      "{'loss': 3.2796, 'grad_norm': 12.419483184814453, 'learning_rate': 5.2e-06, 'epoch': 1.88}\n",
      "{'loss': 3.2822, 'grad_norm': 11.650195121765137, 'learning_rate': 5.15e-06, 'epoch': 1.88}\n",
      "{'loss': 3.2195, 'grad_norm': 11.225675582885742, 'learning_rate': 5.1e-06, 'epoch': 1.88}\n",
      "{'loss': 3.2342, 'grad_norm': 14.215309143066406, 'learning_rate': 5.050000000000001e-06, 'epoch': 1.88}\n",
      "{'loss': 3.2559, 'grad_norm': 11.05266284942627, 'learning_rate': 5e-06, 'epoch': 1.88}\n",
      " 90%|███████████████████████████████▌   | 27000/30000 [6:27:06<38:18,  1.31it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.89s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.68s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.340308, 'eval_rouge-2': 7.537774000000001, 'eval_rouge-l': 26.47111, 'eval_bleu-4': 0.03567365874979823, 'eval_runtime': 13.7147, 'eval_samples_per_second': 3.646, 'eval_steps_per_second': 0.292, 'epoch': 1.88}\n",
      " 90%|███████████████████████████████▌   | 27000/30000 [6:27:20<38:18,  1.31it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.60s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-27000\n",
      "tokenizer config file saved in ./output/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-27000/special_tokens_map.json\n",
      "{'loss': 3.2279, 'grad_norm': 10.36025619506836, 'learning_rate': 4.950000000000001e-06, 'epoch': 1.89}\n",
      "{'loss': 3.2174, 'grad_norm': 11.090676307678223, 'learning_rate': 4.9000000000000005e-06, 'epoch': 1.89}\n",
      "{'loss': 3.2043, 'grad_norm': 11.292046546936035, 'learning_rate': 4.85e-06, 'epoch': 1.89}\n",
      "{'loss': 3.2533, 'grad_norm': 11.172476768493652, 'learning_rate': 4.800000000000001e-06, 'epoch': 1.89}\n",
      "{'loss': 3.3077, 'grad_norm': 11.494681358337402, 'learning_rate': 4.75e-06, 'epoch': 1.9}\n",
      "{'loss': 3.2483, 'grad_norm': 13.215799331665039, 'learning_rate': 4.7e-06, 'epoch': 1.9}\n",
      "{'loss': 3.2275, 'grad_norm': 10.60759449005127, 'learning_rate': 4.65e-06, 'epoch': 1.9}\n",
      "{'loss': 3.2051, 'grad_norm': 11.418702125549316, 'learning_rate': 4.6e-06, 'epoch': 1.9}\n",
      "{'loss': 3.2224, 'grad_norm': 12.459108352661133, 'learning_rate': 4.5500000000000005e-06, 'epoch': 1.9}\n",
      "{'loss': 3.2697, 'grad_norm': 12.466504096984863, 'learning_rate': 4.5e-06, 'epoch': 1.91}\n",
      "{'loss': 3.2116, 'grad_norm': 11.325064659118652, 'learning_rate': 4.45e-06, 'epoch': 1.91}\n",
      "{'loss': 3.2347, 'grad_norm': 11.414607048034668, 'learning_rate': 4.4e-06, 'epoch': 1.91}\n",
      "{'loss': 3.2367, 'grad_norm': 10.543966293334961, 'learning_rate': 4.35e-06, 'epoch': 1.91}\n",
      "{'loss': 3.3321, 'grad_norm': 13.97522258758545, 'learning_rate': 4.2999999999999995e-06, 'epoch': 1.91}\n",
      "{'loss': 3.2325, 'grad_norm': 12.80069637298584, 'learning_rate': 4.250000000000001e-06, 'epoch': 1.92}\n",
      "{'loss': 3.251, 'grad_norm': 12.470833778381348, 'learning_rate': 4.2000000000000004e-06, 'epoch': 1.92}\n",
      "{'loss': 3.2784, 'grad_norm': 11.804864883422852, 'learning_rate': 4.15e-06, 'epoch': 1.92}\n",
      "{'loss': 3.2774, 'grad_norm': 12.514765739440918, 'learning_rate': 4.1000000000000006e-06, 'epoch': 1.92}\n",
      "{'loss': 3.279, 'grad_norm': 9.93677806854248, 'learning_rate': 4.05e-06, 'epoch': 1.92}\n",
      "{'loss': 3.1616, 'grad_norm': 12.474470138549805, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.93}\n",
      " 92%|████████████████████████████████▏  | 27600/30000 [6:35:25<34:49,  1.15it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.36s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.794216, 'eval_rouge-2': 8.062024, 'eval_rouge-l': 25.824161999999998, 'eval_bleu-4': 0.039085103966416984, 'eval_runtime': 13.1461, 'eval_samples_per_second': 3.803, 'eval_steps_per_second': 0.304, 'epoch': 1.93}\n",
      " 92%|████████████████████████████████▏  | 27600/30000 [6:35:38<34:49,  1.15it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.39s/it]\u001b[A\n",
      "{'loss': 3.1785, 'grad_norm': 10.72576904296875, 'learning_rate': 3.95e-06, 'epoch': 1.93}\n",
      "{'loss': 3.2398, 'grad_norm': 11.661952018737793, 'learning_rate': 3.9e-06, 'epoch': 1.93}\n",
      "{'loss': 3.2559, 'grad_norm': 11.898714065551758, 'learning_rate': 3.85e-06, 'epoch': 1.93}\n",
      "{'loss': 3.2401, 'grad_norm': 11.614935874938965, 'learning_rate': 3.8e-06, 'epoch': 1.94}\n",
      "{'loss': 3.276, 'grad_norm': 10.640376091003418, 'learning_rate': 3.75e-06, 'epoch': 1.94}\n",
      "{'loss': 3.2573, 'grad_norm': 11.653457641601562, 'learning_rate': 3.7e-06, 'epoch': 1.94}\n",
      "{'loss': 3.3307, 'grad_norm': 12.03730583190918, 'learning_rate': 3.6499999999999998e-06, 'epoch': 1.94}\n",
      "{'loss': 3.2564, 'grad_norm': 10.89045238494873, 'learning_rate': 3.6e-06, 'epoch': 1.94}\n",
      "{'loss': 3.2861, 'grad_norm': 11.240723609924316, 'learning_rate': 3.55e-06, 'epoch': 1.95}\n",
      "{'loss': 3.2426, 'grad_norm': 12.125177383422852, 'learning_rate': 3.5000000000000004e-06, 'epoch': 1.95}\n",
      "{'loss': 3.2999, 'grad_norm': 12.386804580688477, 'learning_rate': 3.4500000000000004e-06, 'epoch': 1.95}\n",
      "{'loss': 3.2503, 'grad_norm': 11.304572105407715, 'learning_rate': 3.4000000000000005e-06, 'epoch': 1.95}\n",
      "{'loss': 3.2564, 'grad_norm': 10.179874420166016, 'learning_rate': 3.3500000000000005e-06, 'epoch': 1.95}\n",
      "{'loss': 3.2019, 'grad_norm': 11.142191886901855, 'learning_rate': 3.3e-06, 'epoch': 1.96}\n",
      "{'loss': 3.2266, 'grad_norm': 10.95734977722168, 'learning_rate': 3.2500000000000002e-06, 'epoch': 1.96}\n",
      "{'loss': 3.3033, 'grad_norm': 10.133771896362305, 'learning_rate': 3.2000000000000003e-06, 'epoch': 1.96}\n",
      "{'loss': 3.2587, 'grad_norm': 11.001412391662598, 'learning_rate': 3.1500000000000003e-06, 'epoch': 1.96}\n",
      "{'loss': 3.2465, 'grad_norm': 12.326892852783203, 'learning_rate': 3.1e-06, 'epoch': 1.96}\n",
      "{'loss': 3.2436, 'grad_norm': 9.835280418395996, 'learning_rate': 3.05e-06, 'epoch': 1.97}\n",
      "{'loss': 3.2339, 'grad_norm': 12.005415916442871, 'learning_rate': 3e-06, 'epoch': 1.97}\n",
      " 94%|████████████████████████████████▉  | 28200/30000 [6:43:44<24:15,  1.24it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.84s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:09<00:03,  3.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.845668, 'eval_rouge-2': 7.763796000000001, 'eval_rouge-l': 25.744428000000003, 'eval_bleu-4': 0.03674952213762546, 'eval_runtime': 15.7926, 'eval_samples_per_second': 3.166, 'eval_steps_per_second': 0.253, 'epoch': 1.97}\n",
      " 94%|████████████████████████████████▉  | 28200/30000 [6:43:59<24:15,  1.24it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:12<00:00,  3.27s/it]\u001b[A\n",
      "{'loss': 3.2131, 'grad_norm': 11.32709789276123, 'learning_rate': 2.95e-06, 'epoch': 1.97}\n",
      "{'loss': 3.2583, 'grad_norm': 10.344637870788574, 'learning_rate': 2.9e-06, 'epoch': 1.97}\n",
      "{'loss': 3.2438, 'grad_norm': 10.874490737915039, 'learning_rate': 2.8500000000000002e-06, 'epoch': 1.97}\n",
      "{'loss': 3.2127, 'grad_norm': 12.114192962646484, 'learning_rate': 2.8000000000000003e-06, 'epoch': 1.98}\n",
      "{'loss': 3.2727, 'grad_norm': 11.580157279968262, 'learning_rate': 2.7500000000000004e-06, 'epoch': 1.98}\n",
      "{'loss': 3.2501, 'grad_norm': 11.617938041687012, 'learning_rate': 2.7e-06, 'epoch': 1.98}\n",
      "{'loss': 3.2408, 'grad_norm': 11.535820007324219, 'learning_rate': 2.65e-06, 'epoch': 1.98}\n",
      "{'loss': 3.2207, 'grad_norm': 13.725013732910156, 'learning_rate': 2.6e-06, 'epoch': 1.99}\n",
      "{'loss': 3.2563, 'grad_norm': 11.50049114227295, 'learning_rate': 2.55e-06, 'epoch': 1.99}\n",
      "{'loss': 3.1941, 'grad_norm': 12.64366626739502, 'learning_rate': 2.5e-06, 'epoch': 1.99}\n",
      "{'loss': 3.2253, 'grad_norm': 11.95771312713623, 'learning_rate': 2.4500000000000003e-06, 'epoch': 1.99}\n",
      "{'loss': 3.2493, 'grad_norm': 12.118118286132812, 'learning_rate': 2.4000000000000003e-06, 'epoch': 1.99}\n",
      "{'loss': 3.2031, 'grad_norm': 11.3896484375, 'learning_rate': 2.35e-06, 'epoch': 2.0}\n",
      "{'loss': 3.2536, 'grad_norm': 11.188420295715332, 'learning_rate': 2.3e-06, 'epoch': 2.0}\n",
      "{'loss': 3.2125, 'grad_norm': 11.830965042114258, 'learning_rate': 2.25e-06, 'epoch': 2.0}\n",
      "{'loss': 3.2378, 'grad_norm': 12.52100658416748, 'learning_rate': 2.2e-06, 'epoch': 2.0}\n",
      "{'loss': 3.207, 'grad_norm': 10.24261474609375, 'learning_rate': 2.1499999999999997e-06, 'epoch': 2.0}\n",
      "{'loss': 3.2296, 'grad_norm': 11.284436225891113, 'learning_rate': 2.1000000000000002e-06, 'epoch': 2.01}\n",
      "{'loss': 3.2253, 'grad_norm': 11.972527503967285, 'learning_rate': 2.0500000000000003e-06, 'epoch': 2.01}\n",
      "{'loss': 3.1827, 'grad_norm': 10.646093368530273, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.01}\n",
      " 96%|█████████████████████████████████▌ | 28800/30000 [6:52:04<16:48,  1.19it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.76s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.57s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.968289999999996, 'eval_rouge-2': 8.298079999999999, 'eval_rouge-l': 26.190146000000006, 'eval_bleu-4': 0.0392775296260136, 'eval_runtime': 12.905, 'eval_samples_per_second': 3.874, 'eval_steps_per_second': 0.31, 'epoch': 2.01}\n",
      " 96%|█████████████████████████████████▌ | 28800/30000 [6:52:17<16:48,  1.19it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.46s/it]\u001b[A\n",
      "{'loss': 3.2178, 'grad_norm': 12.645853042602539, 'learning_rate': 1.95e-06, 'epoch': 2.01}\n",
      "{'loss': 3.2551, 'grad_norm': 10.649312973022461, 'learning_rate': 1.9e-06, 'epoch': 2.01}\n",
      "{'loss': 3.2628, 'grad_norm': 11.946188926696777, 'learning_rate': 1.85e-06, 'epoch': 2.02}\n",
      "{'loss': 3.2062, 'grad_norm': 11.523397445678711, 'learning_rate': 1.8e-06, 'epoch': 2.02}\n",
      "{'loss': 3.2788, 'grad_norm': 11.884528160095215, 'learning_rate': 1.7500000000000002e-06, 'epoch': 2.02}\n",
      "{'loss': 3.2525, 'grad_norm': 12.222158432006836, 'learning_rate': 1.7000000000000002e-06, 'epoch': 2.02}\n",
      "{'loss': 3.2012, 'grad_norm': 12.21816349029541, 'learning_rate': 1.65e-06, 'epoch': 2.03}\n",
      "{'loss': 3.2025, 'grad_norm': 11.150625228881836, 'learning_rate': 1.6000000000000001e-06, 'epoch': 2.03}\n",
      "{'loss': 3.2395, 'grad_norm': 11.458353996276855, 'learning_rate': 1.55e-06, 'epoch': 2.03}\n",
      "{'loss': 3.2693, 'grad_norm': 11.904590606689453, 'learning_rate': 1.5e-06, 'epoch': 2.03}\n",
      "{'loss': 3.1712, 'grad_norm': 12.033742904663086, 'learning_rate': 1.45e-06, 'epoch': 2.03}\n",
      "{'loss': 3.2598, 'grad_norm': 12.568875312805176, 'learning_rate': 1.4000000000000001e-06, 'epoch': 2.04}\n",
      "{'loss': 3.2606, 'grad_norm': 11.43804931640625, 'learning_rate': 1.35e-06, 'epoch': 2.04}\n",
      "{'loss': 3.1398, 'grad_norm': 12.684113502502441, 'learning_rate': 1.3e-06, 'epoch': 2.04}\n",
      "{'loss': 3.2501, 'grad_norm': 12.387738227844238, 'learning_rate': 1.25e-06, 'epoch': 2.04}\n",
      "{'loss': 3.2827, 'grad_norm': 13.618085861206055, 'learning_rate': 1.2000000000000002e-06, 'epoch': 2.04}\n",
      "{'loss': 3.241, 'grad_norm': 11.931906700134277, 'learning_rate': 1.15e-06, 'epoch': 2.05}\n",
      "{'loss': 3.249, 'grad_norm': 11.072691917419434, 'learning_rate': 1.1e-06, 'epoch': 2.05}\n",
      "{'loss': 3.2763, 'grad_norm': 12.057510375976562, 'learning_rate': 1.0500000000000001e-06, 'epoch': 2.05}\n",
      "{'loss': 3.2186, 'grad_norm': 11.303644180297852, 'learning_rate': 1.0000000000000002e-06, 'epoch': 2.05}\n",
      " 98%|██████████████████████████████████▎| 29400/30000 [7:00:22<07:53,  1.27it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.82s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.74s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.392006, 'eval_rouge-2': 7.448114, 'eval_rouge-l': 25.75551, 'eval_bleu-4': 0.03512378398177923, 'eval_runtime': 13.8829, 'eval_samples_per_second': 3.602, 'eval_steps_per_second': 0.288, 'epoch': 2.05}\n",
      " 98%|██████████████████████████████████▎| 29400/30000 [7:00:36<07:53,  1.27it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.43s/it]\u001b[A\n",
      "{'loss': 3.2937, 'grad_norm': 11.300211906433105, 'learning_rate': 9.5e-07, 'epoch': 2.05}\n",
      "{'loss': 3.2228, 'grad_norm': 10.796310424804688, 'learning_rate': 9e-07, 'epoch': 2.06}\n",
      "{'loss': 3.2426, 'grad_norm': 10.81424617767334, 'learning_rate': 8.500000000000001e-07, 'epoch': 2.06}\n",
      "{'loss': 3.2619, 'grad_norm': 12.641682624816895, 'learning_rate': 8.000000000000001e-07, 'epoch': 2.06}\n",
      "{'loss': 3.2443, 'grad_norm': 10.381216049194336, 'learning_rate': 7.5e-07, 'epoch': 2.06}\n",
      "{'loss': 3.1941, 'grad_norm': 10.935746192932129, 'learning_rate': 7.000000000000001e-07, 'epoch': 2.06}\n",
      "{'loss': 3.2409, 'grad_norm': 10.814090728759766, 'learning_rate': 6.5e-07, 'epoch': 2.07}\n",
      "{'loss': 3.1818, 'grad_norm': 12.142722129821777, 'learning_rate': 6.000000000000001e-07, 'epoch': 2.07}\n",
      "{'loss': 3.224, 'grad_norm': 11.660593032836914, 'learning_rate': 5.5e-07, 'epoch': 2.07}\n",
      "{'loss': 3.2607, 'grad_norm': 12.7196044921875, 'learning_rate': 5.000000000000001e-07, 'epoch': 2.07}\n",
      "{'loss': 3.2469, 'grad_norm': 12.19759464263916, 'learning_rate': 4.5e-07, 'epoch': 2.08}\n",
      "{'loss': 3.2087, 'grad_norm': 13.564778327941895, 'learning_rate': 4.0000000000000003e-07, 'epoch': 2.08}\n",
      "{'loss': 3.2004, 'grad_norm': 11.14571762084961, 'learning_rate': 3.5000000000000004e-07, 'epoch': 2.08}\n",
      "{'loss': 3.2265, 'grad_norm': 11.988494873046875, 'learning_rate': 3.0000000000000004e-07, 'epoch': 2.08}\n",
      "{'loss': 3.2413, 'grad_norm': 11.416121482849121, 'learning_rate': 2.5000000000000004e-07, 'epoch': 2.08}\n",
      "{'loss': 3.2814, 'grad_norm': 11.21788215637207, 'learning_rate': 2.0000000000000002e-07, 'epoch': 2.09}\n",
      "{'loss': 3.3051, 'grad_norm': 11.264784812927246, 'learning_rate': 1.5000000000000002e-07, 'epoch': 2.09}\n",
      "{'loss': 3.1781, 'grad_norm': 10.821403503417969, 'learning_rate': 1.0000000000000001e-07, 'epoch': 2.09}\n",
      "{'loss': 3.1932, 'grad_norm': 10.746430397033691, 'learning_rate': 5.0000000000000004e-08, 'epoch': 2.09}\n",
      "{'loss': 3.2503, 'grad_norm': 12.496112823486328, 'learning_rate': 0.0, 'epoch': 2.09}\n",
      "100%|███████████████████████████████████| 30000/30000 [7:08:41<00:00,  1.23it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.86s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.65s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.06832, 'eval_rouge-2': 7.527192, 'eval_rouge-l': 25.275548000000004, 'eval_bleu-4': 0.03591215603630905, 'eval_runtime': 15.7375, 'eval_samples_per_second': 3.177, 'eval_steps_per_second': 0.254, 'epoch': 2.09}\n",
      "100%|███████████████████████████████████| 30000/30000 [7:08:56<00:00,  1.23it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.42s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-30000\n",
      "tokenizer config file saved in ./output/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-30000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 25736.9298, 'train_samples_per_second': 9.325, 'train_steps_per_second': 1.166, 'train_loss': 3.3142800130208334, 'epoch': 2.09}\n",
      "100%|███████████████████████████████████| 30000/30000 [7:08:56<00:00,  1.17it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [04:33<00:00,  4.08s/it]\n"
     ]
    }
   ],
   "source": [
    "!python finetune_hf.py data/AdvertiseGen_fix /home/ubuntu/models/chatglm3-6b configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:04<00:00,  1.66it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "这款连衣裙，采用套头设计，方便穿脱，舒适不紧勒，穿着更加自由自在。不规则的衣摆，结合褶皱的网纱拼接，层次感丰富，视觉上更加显瘦，让整体更加有设计感。领口木耳边设计，精致可爱，更显甜美俏皮。袖口压褶设计，个性十足。领口拉链设计，方便穿脱。\n"
     ]
    }
   ],
   "source": [
    "!python inference_hf.py output/checkpoint-30000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
