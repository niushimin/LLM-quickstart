

<div align="center">
  <h2>AI 大模型微调训练营课程总结</h2>
  </br>
</div>

​	非常高兴极客时间能在继《AI 大模型应用开发训练营》之后，又推出大模型相关的进阶训练营《AI 大模型微调训练营》。两个训练营我都有幸参加了，在此《AI 大模型微调训练营》毕业之际，想总结一下整个学习过程的一些收获以及对大模型发展的感悟。

​	我非常认可彭老师提出的 AI 大模型的四阶技术：提示工程、智能体、大模型微调、大模型训练。整个四阶技术从上到下逐层深入，适用于不同阶段的应用场景。《AI 大模型应用开发训练营》关注于提示工程和智能体，提示工程主要聚焦在使用提示词完成任务，介绍了CoT、SC、ToT等一些提示词技巧的原理；智能体主要聚焦通过LangChain开发框架快速的构建基于大模型的 AI 应用，比如实现基于 RAG 架构的智能聊天机器人，基于 ReAct 思想的 AI 智能体。而《AI 大模型微调训练营》关注于大模型微调，介绍了大模型微调的一些方法并结合实战讲这些方法落实到真实的微调任务中。大模型训练则需要更扎实的机器学习和深度学习基础才能掌握，希望以后能够通过自己的学习训练自己的大模型。

​	回顾一下整个大模型的技术发展历史，自注意力机制和Transformer架构的引入，解决了之前RNN处理时序数据的一些问题：信息丢失和计算效率低，使得训练大参数的模型成为可能。同时基于Transformer架构又衍生出了不同的技术路线：以GPT为代表的Decoder-only路线；以BERT为代表的Encoder-only路线；以T5为代表的Encoder-Decoder路线。不同的技术路线选择体现了相关研究人员的指挥，在早期阶段，BERT采用预训练和指定任务的微调吸引了极大的关注。后来GPT证明只通过预训练也可以达到不错的效果，微调则可以用于进一步提升了模型在私有数据集上的表现。

​	伴随着大模型的发展，Hugging Face逐渐发展壮大，从最初提供BERT的开源版本实现到现在集模型，数据集，模型空间以及一些开发库为一体的大模型开发社区。Hugging Face的Transformers库封装了TensorFlow和PyTorch等深度学习框架，极大地降低了研究和开发的门槛，简化Transformer模型的使用，提供了上百种预训练模型和丰富的工具集，Hugging Face的PEFT库则提供了模型微调的工具的方法，使得研究人员和开发者能够快速实现模型部署和微调。

​	实战Transformers模型训练章节深入探讨了如何利用Transformers库进行大模型的训练，包括数据准备、模型配置、训练过程优化等关键步骤。这一过程中，数据预处理的重要性、合适的模型架构和参数选择、以及训练技巧的应用都是提高模型性能和效率的关键因素。同时，模型训练中可能遇到的挑战，如过拟合、计算资源限制等，也需要通过合理的策略来应对。

​	大模型高效微调章节介绍了各种微调技术分类及原理，从原理上理解微调的工作原理，通过调整微调过程中的学习率和其他超参数，模型能够在保持泛化能力的同时，提高在特定任务上的表现。此外，少样本学习、元学习等高级微调技术在数据受限的情况下进一步提高了模型的适应性和性能，同时使用Hugging Face的PEFT库使用LoRA和QLoRA的方法对ChatGLM3-6B进行微调实战，真正上手基于开源模型进行模型微调。

​	基于人类反馈的强化学习章节(RLHF)学习了如何训练大模型对齐的人类的价值观，生成人类满意的答案，主要分为三个步骤：1. 预训练一个语言模型 (LM) ；2. 聚合问答数据并训练一个奖励模型 (Reward Model，RM) ；3.用强化学习 (RL)方式微调 LM。收集人类偏好数据的质量和数量决定了 RLHF 系统性能的上限，与此同时合理的奖励算法也是需要研究的一个课题。

​	基于稀疏门控网络的混合专家模型(MoEs)则是最近大模型发展的又一个方向。随着Mixtral 8x7B的推出，一种称为混合专家模型 (Mixed Expert Models，简称 MoEs)的 Transformer 模型在开源人工智能社区引起了广泛关注。混合专家模型与稠密模型相比，预训练速度更快，与具有相同参数数量的模型相比，具有更快的推理速度。但在微调方面存在诸多挑战，但近期的研究表明，对混合专家模型进行指令调优具有很大的潜力。

​	最后介绍了大模型分布式训练框架DeepSpeed，展示了如何通过分布式训练技术实现大模型的高效训练。通过介绍论文的内容了解到实现分布式训练的数据并行，模型并行以及流水线并线的相关配置(ZeRO-1、ZeRO-2、ZeRO-3、ZeRO-offload、ZeRO-infinity)，最后通过实战的方式进行了DeepSpeed单机单卡训练（训练条件有限，多卡训练原理类似）。同时介绍了大模型在国产硬件上的适配，随着大模型的发展，算力需求也会逐步增大，在当前的国际环境下，为了保证国内的大模型发展不受制于人，需要了解国内的AI算力硬件设施及平台，做好规划，未雨绸缪。

​	总之，通过训练营的学习，了解和学习到了很多大模型相关技术和原理，但这只是开始，训练营不可能覆盖全部的内容，并且大模型的技术还在不断的快速发展中。需要保持对大模型技术的发展的敏感度，时刻关注大模型领域的相关新闻，多看，多学，多练，早日成为大模型领域的佼佼者。大模型技术的发展不仅在学术研究领域取得了显著成就，在工业应用中也展现出了巨大的潜力和价值。随着技术的不断进步和创新，大模型将在未来继续扮演着越来越重要的角色，推动人工智能技术向更高的目标前进。每个人都应该了解和学习大模型技术！





























